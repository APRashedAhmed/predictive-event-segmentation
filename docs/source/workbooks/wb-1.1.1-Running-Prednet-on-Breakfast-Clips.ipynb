{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1.1 Running Prednet on Breakfast Clips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [watermark](https://github.com/rasbt/watermark) to see the state of the machine and environment that's running the notebook. To make sense of the options, take a look at the [usage](https://github.com/rasbt/watermark#usage) section of the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 06 2020 15:57:38 \n",
      "\n",
      "CPython 3.6.10\n",
      "IPython 7.12.0\n",
      "\n",
      "torch 1.2.0\n",
      "torchvision 0.1.8\n",
      "cv2 3.4.2\n",
      "h5py 2.8.0\n",
      "pandas 1.0.1\n",
      "matplotlib 3.1.3\n",
      "seaborn 0.10.0\n",
      "jupyterlab 1.2.6\n",
      "lab 0+untagged.38.g6a19aca.dirty\n",
      "\n",
      "compiler   : GCC 7.3.0\n",
      "system     : Linux\n",
      "release    : 4.15.0-76-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n",
      "Git hash   : 6a19aca9e16ed91aaf852f0914eb45b18ea68d92\n",
      "Git branch : master\n"
     ]
    }
   ],
   "source": [
    "# Load `watermark` extension\n",
    "%load_ext watermark\n",
    "# Display the status of the machine and packages. Add more as necessary.\n",
    "%watermark -v -n -m -g -b -t -p torch,torchvision,cv2,h5py,pandas,matplotlib,seaborn,jupyterlab,lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [autoreload](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html) which will always reload modules marked with `%aimport`.\n",
    "\n",
    "This behavior can be inverted by running `autoreload 2` which will set everything to be auto-reloaded *except* for modules marked with `%aimport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `autoreload` extension\n",
    "%load_ext autoreload\n",
    "# Set autoreload behavior\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `matplotlib` in one of the more `jupyter`-friendly [rich-output modes](https://ipython.readthedocs.io/en/stable/interactive/plotting.html). Some options (that may or may not have worked) are `inline`, `notebook`, and `gtk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the matplotlib mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the GPU\n",
    "\n",
    "Make sure we aren't greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar  6 15:58:41 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN Xp            Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 28%   49C    P2    75W / 250W |   9399MiB / 12196MiB |     31%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN Xp            Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 39%   63C    P2    89W / 250W |  12037MiB / 12196MiB |      7%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN Xp            Off  | 00000000:08:00.0 Off |                  N/A |\n",
      "| 43%   69C    P2    90W / 250W |  12037MiB / 12196MiB |      7%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  TITAN Xp            Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 30%   42C    P8     9W / 250W |      0MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  TITAN Xp            Off  | 00000000:84:00.0 Off |                  N/A |\n",
      "| 23%   31C    P8     8W / 250W |      0MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  TITAN Xp            Off  | 00000000:85:00.0 Off |                  N/A |\n",
      "| 36%   60C    P2   114W / 250W |  12037MiB / 12196MiB |     44%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  TITAN Xp            Off  | 00000000:88:00.0 Off |                  N/A |\n",
      "| 31%   51C    P2    79W / 250W |  10131MiB / 12196MiB |     14%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  TITAN Xp            Off  | 00000000:89:00.0 Off |                  N/A |\n",
      "| 32%   51C    P2    63W / 250W |   3309MiB / 12196MiB |      9%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     13544      C   python                                      9389MiB |\n",
      "|    1     26564      C   python                                     12027MiB |\n",
      "|    2      7023      C   python                                     12027MiB |\n",
      "|    5      8557      C   python                                     12027MiB |\n",
      "|    6      9519      C   python                                     10121MiB |\n",
      "|    7     27219      C   python                                      3299MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local imports that may or may not be autoreloaded. This section contains things that will likely have to be re-imported multiple times, and have additions or subtractions made throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants to be used throughout the package\n",
    "%aimport lab\n",
    "import lab\n",
    "%aimport lab.index\n",
    "from lab.index import DIR_DATA_INT, DIR_DATA_RAW\n",
    "%aimport lab.breakfast\n",
    "import lab.breakfast as bk\n",
    "%aimport lab.breakfast.constants\n",
    "from lab.breakfast.constants import SEED\n",
    "# Import the data subdirectories\n",
    "%aimport lab.breakfast.index\n",
    "from lab.breakfast.index import (DIR_BREAKFAST, \n",
    "                                 DIR_BREAKFAST_DATA, \n",
    "                                 DIR_COARSE_SEG, \n",
    "                                 DIR_FINE_SEG,\n",
    "                                 DIR_BK_WEIGHTS,\n",
    "                                 DIR_BK_CHECKPOINTS,\n",
    "                                )\n",
    "%aimport lab.breakfast.prednet\n",
    "from lab.breakfast.prednet import PredNet\n",
    "%aimport lab.breakfast.dataloader\n",
    "from lab.breakfast.dataloader import Breakfast64DimFVDataset, BreakfastI3DFVDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set [seaborn defaults](https://seaborn.pydata.org/generated/seaborn.set.html) for matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data\n",
    "\n",
    "See [this](https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets/50544887#50544887) stackoverflow post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Breakfast64DimFVDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "ds_length = len(ds)\n",
    "indices = list(range(ds_length))\n",
    "n_test = 128\n",
    "batch_size = 256\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[n_test:], indices[:n_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = DataLoader(ds, batch_size=batch_size, sampler=train_sampler)\n",
    "test_loader = DataLoader(ds, batch_size=batch_size, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24_pancake_webcam01_start_6219_seed_117\n",
      "37_friedegg_webcam01_start_1041_seed_117\n",
      "41_salat_webcam02_start_152_seed_117\n",
      "43_pancake_cam02_start_1767_seed_117\n",
      "46_tea_webcam01_start_394_seed_117\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, path) in enumerate(train_loader):\n",
    "        if batch_idx is 1:\n",
    "            print(Path(path[0]).stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Pass at Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport lab.breakfast.prednet\n",
    "from lab.breakfast.prednet import PredNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "A_channels = (64, 128, 256)\n",
    "R_channels = (64, 128, 256)\n",
    "lr = 0.001 # if epoch < 75 else 0.0001\n",
    "nt = 64 # num of time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_loss_weights = Variable(torch.FloatTensor([[1.], [0.], [0.]]).cuda())\n",
    "time_loss_weights = 1./(nt - 1) * torch.ones(nt, 1)\n",
    "time_loss_weights[0] = 0\n",
    "time_loss_weights = Variable(time_loss_weights.cuda())\n",
    "\n",
    "# layer_loss_weights = Variable(torch.FloatTensor([[1.], [0.], [0.], [0.]]))\n",
    "# time_loss_weights = 1./(nt - 1) * torch.ones(nt, 1)\n",
    "# time_loss_weights[0] = 0\n",
    "# time_loss_weights = Variable(time_loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU.\n"
     ]
    }
   ],
   "source": [
    "model = PredNet(R_channels, A_channels, output_mode='error')\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU.')\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def lr_scheduler(optimizer, epoch):\n",
    "    if epoch < num_epochs //2:\n",
    "        return optimizer\n",
    "    else:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.0001\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = DIR_BK_CHECKPOINTS / 'checkpoint.tar'\n",
    "path_weights = DIR_BK_WEIGHTS / 'training.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, step: 0/158, errors: 1.8063452243804932\n",
      "Epoch: 0/10, step: 50/158, errors: 1.7441104650497437\n",
      "Epoch: 0/10, step: 100/158, errors: 1.6239502429962158\n",
      "Epoch: 0/10, step: 150/158, errors: 1.5582005977630615\n",
      "Epoch: 1/10, step: 0/158, errors: 1.5833419561386108\n",
      "Epoch: 1/10, step: 50/158, errors: 1.514613151550293\n",
      "Epoch: 1/10, step: 100/158, errors: 1.4902374744415283\n",
      "Epoch: 1/10, step: 150/158, errors: 1.4473806619644165\n",
      "Epoch: 2/10, step: 0/158, errors: 1.436287760734558\n",
      "Epoch: 2/10, step: 50/158, errors: 1.3997976779937744\n",
      "Epoch: 2/10, step: 100/158, errors: 1.3535420894622803\n",
      "Epoch: 2/10, step: 150/158, errors: 1.3311622142791748\n",
      "Epoch: 3/10, step: 0/158, errors: 1.3118345737457275\n",
      "Epoch: 3/10, step: 50/158, errors: 1.2846627235412598\n",
      "Epoch: 3/10, step: 100/158, errors: 1.2719541788101196\n",
      "Epoch: 3/10, step: 150/158, errors: 1.236175298690796\n",
      "Epoch: 4/10, step: 0/158, errors: 1.2455084323883057\n",
      "Epoch: 4/10, step: 50/158, errors: 1.2225968837738037\n",
      "Epoch: 4/10, step: 100/158, errors: 1.1840271949768066\n",
      "Epoch: 4/10, step: 150/158, errors: 1.1740503311157227\n",
      "Epoch: 5/10, step: 0/158, errors: 1.1692109107971191\n",
      "Epoch: 5/10, step: 50/158, errors: 1.1719024181365967\n",
      "Epoch: 5/10, step: 100/158, errors: 1.1586198806762695\n",
      "Epoch: 5/10, step: 150/158, errors: 1.1909313201904297\n",
      "Epoch: 6/10, step: 0/158, errors: 1.1617703437805176\n",
      "Epoch: 6/10, step: 50/158, errors: 1.1535394191741943\n",
      "Epoch: 6/10, step: 100/158, errors: 1.163571834564209\n",
      "Epoch: 6/10, step: 150/158, errors: 1.1772565841674805\n",
      "Epoch: 7/10, step: 0/158, errors: 1.1588621139526367\n",
      "Epoch: 7/10, step: 50/158, errors: 1.1540769338607788\n",
      "Epoch: 7/10, step: 100/158, errors: 1.1704702377319336\n",
      "Epoch: 7/10, step: 150/158, errors: 1.1796112060546875\n",
      "Epoch: 8/10, step: 0/158, errors: 1.1658849716186523\n",
      "Epoch: 8/10, step: 50/158, errors: 1.1586105823516846\n",
      "Epoch: 8/10, step: 100/158, errors: 1.1628386974334717\n",
      "Epoch: 8/10, step: 150/158, errors: 1.1712158918380737\n",
      "Epoch: 9/10, step: 0/158, errors: 1.1752440929412842\n",
      "Epoch: 9/10, step: 50/158, errors: 1.167525291442871\n",
      "Epoch: 9/10, step: 100/158, errors: 1.1625456809997559\n",
      "Epoch: 9/10, step: 150/158, errors: 1.1494688987731934\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    optimizer = lr_scheduler(optimizer, epoch)\n",
    "    for batch_idx, (data, path) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        errors = model(data) # batch x n_layers x nt\n",
    "        loc_batch = errors.size(0)\n",
    "        errors = torch.mm(errors.view(-1, nt), time_loss_weights) # batch*n_layers x 1\n",
    "        errors = torch.mm(errors.view(loc_batch, -1), layer_loss_weights)\n",
    "        errors = torch.mean(errors, axis=0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        errors.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f'Epoch: {epoch}/{num_epochs}, step: {batch_idx}/{len(ds)//batch_size}, '\n",
    "                  f'errors: {errors.data[0]}')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'batch_idx' : batch_idx,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'errors': errors,\n",
    "            }, str(path_checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), str(path_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running with I3D Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.61 s, sys: 8.44 s, total: 10 s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = BreakfastI3DFVDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "ds_length = len(ds)\n",
    "indices = list(range(ds_length))\n",
    "batch_size = 128\n",
    "n_test = 128\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[n_test:], indices[:n_test]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = DataLoader(ds, batch_size=batch_size, sampler=train_sampler)\n",
    "test_loader = DataLoader(ds, batch_size=batch_size, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, layer_loss_weights, time_loss_weights\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU.\n",
      "Epoch: 0/100, test error: 1.233896255493164\n",
      "Epoch: 1/100, test error: 0.8275737762451172\n",
      "Epoch: 2/100, test error: 0.6741371750831604\n",
      "Epoch: 3/100, test error: 0.5927122235298157\n",
      "Epoch: 4/100, test error: 0.5393632650375366\n",
      "Epoch: 5/100, test error: 0.5091759562492371\n",
      "Epoch: 6/100, test error: 0.4874243140220642\n",
      "Epoch: 7/100, test error: 0.45570212602615356\n",
      "Epoch: 8/100, test error: 0.42265135049819946\n",
      "Epoch: 9/100, test error: 0.4050993323326111\n",
      "Epoch: 10/100, test error: 0.3980809152126312\n",
      "Epoch: 11/100, test error: 0.3884073495864868\n",
      "Epoch: 12/100, test error: 0.38043126463890076\n",
      "Epoch: 13/100, test error: 0.37487083673477173\n",
      "Epoch: 14/100, test error: 0.37232667207717896\n",
      "Epoch: 15/100, test error: 0.3663807213306427\n",
      "Epoch: 16/100, test error: 0.3616701364517212\n",
      "Epoch: 17/100, test error: 0.3601449131965637\n",
      "Epoch: 18/100, test error: 0.35761237144470215\n",
      "Epoch: 19/100, test error: 0.35426682233810425\n",
      "Epoch: 20/100, test error: 0.35477757453918457\n",
      "Epoch: 21/100, test error: 0.35200369358062744\n",
      "Epoch: 22/100, test error: 0.3475354015827179\n",
      "Epoch: 23/100, test error: 0.3450595438480377\n",
      "Epoch: 24/100, test error: 0.3455773890018463\n",
      "Epoch: 25/100, test error: 0.34384483098983765\n",
      "Epoch: 26/100, test error: 0.3432426452636719\n",
      "Epoch: 27/100, test error: 0.873805046081543\n",
      "Epoch: 28/100, test error: 0.806170642375946\n",
      "Epoch: 29/100, test error: 0.782880961894989\n",
      "Epoch: 30/100, test error: 0.9117268323898315\n",
      "Epoch: 31/100, test error: 0.9034321308135986\n",
      "Epoch: 32/100, test error: 0.8865249752998352\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[128, -1]' is invalid for input of size 88",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[128, -1]' is invalid for input of size 88"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_epochs = 100\n",
    "n_layers = 4\n",
    "input_size = 2048\n",
    "nt = 64 # num of time steps\n",
    "A_channels = tuple(input_size // (2**i) for i in range(n_layers))\n",
    "R_channels = tuple(input_size // (2**i) for i in range(n_layers))\n",
    "lr = 0.001 # if epoch < 75 else 0.0001\n",
    "\n",
    "path_checkpoint = DIR_BK_CHECKPOINTS / 'i3d_checkpoint.tar'\n",
    "path_weights = DIR_BK_WEIGHTS / 'i3d_training.pt'\n",
    "\n",
    "layer_loss_weights = Variable(torch.FloatTensor([[1.]] + [[0.]]*(n_layers-1)).cuda())\n",
    "time_loss_weights = 1./(nt - 1) * torch.ones(nt, 1)\n",
    "time_loss_weights[0] = 0\n",
    "time_loss_weights = Variable(time_loss_weights.cuda())\n",
    "\n",
    "model = PredNet(R_channels, A_channels, output_mode='error')\n",
    "if torch.cuda.is_available():\n",
    "    print('Using GPU.')\n",
    "    model.cuda()\n",
    "print(model)\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def lr_scheduler(optimizer, epoch):\n",
    "    if epoch < num_epochs //2:\n",
    "        return optimizer\n",
    "    else:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.0001\n",
    "        return optimizer\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    optimizer = lr_scheduler(optimizer, epoch)\n",
    "    for batch_idx, (data, path) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        errors = model(data) # batch x n_layers x nt\n",
    "        loc_batch = errors.size(0)\n",
    "        errors = torch.mm(errors.view(-1, nt), time_loss_weights) # batch*n_layers x 1\n",
    "        errors = torch.mm(errors.view(loc_batch, -1), layer_loss_weights)\n",
    "        errors = torch.mean(errors, axis=0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        errors.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         if batch_idx % 25 == 0:\n",
    "#             print(f'Epoch: {epoch}/{num_epochs}, step: {batch_idx}/{len(ds)//batch_size}, '\n",
    "#                   f'train error: {errors.data[0]}')\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'batch_idx' : batch_idx,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'errors': errors,\n",
    "#             }, str(path_checkpoint))\n",
    "            \n",
    "    test_errors = []\n",
    "\n",
    "    for data, path in test_loader:\n",
    "        data = Variable(data)\n",
    "        errors = model(data) # batch x n_layers x nt\n",
    "        loc_batch = errors.size(0)\n",
    "        errors = torch.mm(errors.view(-1, nt), time_loss_weights) # batch*n_layers x 1\n",
    "        errors = torch.mm(errors.view(loc_batch, -1), layer_loss_weights)\n",
    "        test_errors.append(torch.mean(errors, axis=0).item())\n",
    "    \n",
    "    test_error = np.mean(test_errors)\n",
    "    print(f'Epoch: {epoch}/{num_epochs}, test error: {test_error}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
