{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.2 Running the CLPN with Batch Size of 4 After Initialization\n",
    "\n",
    "In [wb-2.2.1] I found when initializing the models, path lengths affect whether it can fit in GPU memory. See the following  attempts:\n",
    "\n",
    "- batch_size, max_steps, success\n",
    "- 4, 100, False\n",
    "- 4, 50, False\n",
    "- 4, 20, True\n",
    "- 3, 100, False\n",
    "- 3, 75, True\n",
    "- 2, 100, True\n",
    "\n",
    "However, if initialized with smaller path lengths first and then changed to be larger, the problem disappears. This NB explores what happens if we train starting with `batch_size=4` and `max_steps=20`, and then changing to `max_steps=100`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [watermark](https://github.com/rasbt/watermark) to see the state of the machine and environment that's running the notebook. To make sense of the options, take a look at the [usage](https://github.com/rasbt/watermark#usage) section of the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 01 2020 15:03:02 \n",
      "\n",
      "compiler   : GCC 7.3.0\n",
      "system     : Linux\n",
      "release    : 4.15.0-112-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n",
      "host name  : serrep5\n",
      "Git hash   : 5b0219b19581369b235b6b1f8aa6f3f095ed0b85\n",
      "Git branch : master\n"
     ]
    }
   ],
   "source": [
    "# Load `watermark` extension\n",
    "%load_ext watermark\n",
    "# Display the status of the machine and other non-code related info\n",
    "%watermark -n -m -g -b -t -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [autoreload](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html) which will always reload modules marked with `%aimport`.\n",
    "\n",
    "This behavior can be inverted by running `autoreload 2` which will set everything to be auto-reloaded *except* for modules marked with `%aimport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `autoreload` extension\n",
    "%load_ext autoreload\n",
    "# Set autoreload behavior\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `matplotlib` in one of the more `jupyter`-friendly [rich-output modes](https://ipython.readthedocs.io/en/stable/interactive/plotting.html). Some options (that may or may not have worked) are `inline`, `notebook`, and `gtk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the matplotlib mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conda Env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: conda: command not found\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL.Image  7.2.0\n",
      "networkx   2.5\n",
      "numpy      1.19.1\n",
      "prevseg    0+untagged.64.g5b0219b\n",
      "prednet    0+untagged.74.g57d9b43\n",
      "matplotlib 3.3.1\n",
      "CPython 3.7.7\n",
      "IPython 7.17.0\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import networkx as nx\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import Iterator\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "%aimport prevseg.index\n",
    "import prevseg.index as index\n",
    "%aimport prevseg.schapiro\n",
    "import prevseg.schapiro as sch\n",
    "from prevseg.schapiro import graph, walk\n",
    "\n",
    "%aimport prednet.kitti_settings\n",
    "import prednet.kitti_settings as ks\n",
    "%aimport prednet.prednet_base\n",
    "import prednet.prednet_base as pn\n",
    "%aimport prednet.data_utils\n",
    "import prednet.data_utils as utils\n",
    "\n",
    "# Keep track of versions of everything\n",
    "%watermark -v -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Schapiro Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 14\n",
    "n_pentagons = 3\n",
    "max_steps = 100\n",
    "n_paths = 1\n",
    "source = 1\n",
    "\n",
    "paths_data = list(index.DIR_SCH_FRACTALS_RS.iterdir())\n",
    "np.random.shuffle(paths_data)\n",
    "paths_data = paths_data[:5*n_pentagons]\n",
    "array_data = np.array([np.load(str(path)) for path in paths_data])\n",
    "\n",
    "G = sch.graph.schapiro_graph(n_pentagons=n_pentagons)\n",
    "\n",
    "mapping = {node : path.stem for node, path in zip(range(len(G.nodes)), paths_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 100, 3, 128, 160)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def iter_single_sample(G, mode, max_steps, source=None): \n",
    "    if mode == 'random':\n",
    "        iter_walk = sch.walk.walk_random(G, steps=max_steps, source=source)\n",
    "    elif mode == 'euclidean':\n",
    "        iter_walk = sch.walk.walk_euclidean(G, source=source)\n",
    "    elif mode == 'hamiltonian':\n",
    "        iter_walk = sch.walk.walk_hamiltonian(G, source=source)\n",
    "    else:\n",
    "        raise ValueError('Invalid mode entered')\n",
    "\n",
    "    for sample in iter_walk:\n",
    "        yield array_data[sample[0]], sample[0]\n",
    "\n",
    "def iter_batch_sample(G, mode, max_steps, batch_size, source=None):\n",
    "    iter_batch = zip(*[iter_single_sample(G, mode, max_steps, source=source) \n",
    "                       for _ in range(batch_size)])\n",
    "    for batch in iter_batch:\n",
    "        data, nodes = zip(*batch)\n",
    "        yield data, nodes\n",
    "\n",
    "def iter_batch_dataset(G, mode, max_steps, batch_size, n_paths, source=None):       \n",
    "    for _ in range(n_paths):\n",
    "        data, nodes = zip(*list(iter_batch_sample(\n",
    "            G, mode, max_steps, batch_size, source=source)))\n",
    "        yield np.moveaxis(np.array(data), 0, 1), nodes\n",
    "\n",
    "fine_tune_data_iter = iter_batch_dataset(G, 'random', max_steps, batch_size, n_paths)\n",
    "fine_tune_data, fine_tune_nodes = next(fine_tune_data_iter)\n",
    "fine_tune_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_iter = iter_batch_dataset(G, 'euclidean', None, 1, n_paths, source=source)\n",
    "_, euclidean_nodes = next(euclidean_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 13,\n",
       " 14,\n",
       " 12,\n",
       " 13,\n",
       " 11,\n",
       " 12,\n",
       " 10,\n",
       " 11,\n",
       " 14,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_nodes = list(np.array(euclidean_nodes).reshape(30))\n",
    "euclidean_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 36, 3, 128, 160)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inlcude some non-important steps the start to allow the hidden state to adapt\n",
    "initial_padding = [1,0,1,0,1,0]\n",
    "test_walk_nodes = initial_padding + euclidean_nodes\n",
    "\n",
    "test_data = np.expand_dims(np.array([array_data[n] for n in test_walk_nodes]),0)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sequence Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mapping as follows:\n",
      "{0: '16', 1: '97', 2: '62', 3: '94', 4: '19', 5: '32', 6: '40', 7: '15', 8: '10', 9: '86', 10: '51', 11: '33', 12: '30', 13: '29', 14: '61'}\n",
      "0 (14, 100, 3, 128, 160)\n"
     ]
    }
   ],
   "source": [
    "class ShapiroFractalsDataset(Iterator):\n",
    "    modes = set(('random', 'euclidean', 'hamiltonian'))\n",
    "    def __init__(self, batch_size=32, n_pentagons=3, max_steps=128, n_paths=128,\n",
    "                 mapping=None, mode='random', debug=False, seed=None, shuffle=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.n_pentagons = n_pentagons\n",
    "        self.max_steps = max_steps\n",
    "        self.n_paths = n_paths\n",
    "        self.mapping = mapping\n",
    "        self.mode = mode\n",
    "        self.debug = debug\n",
    "        assert self.mode in self.modes\n",
    "        \n",
    "        self.G = graph.schapiro_graph(n_pentagons=n_pentagons)\n",
    "        \n",
    "        self.load_node_stimuli()\n",
    "        \n",
    "        self.mapping = {node : path.stem\n",
    "                        for node, path in zip(range(len(self.G.nodes)),\n",
    "                                              self.paths_data)}\n",
    "        print(f'Created mapping as follows:\\n{self.mapping}')\n",
    "        \n",
    "        if self.debug:\n",
    "            self.sample_transform = lambda sample : sample\n",
    "        else:\n",
    "            self.sample_transform = lambda sample : self.array_data[sample]\n",
    "        super().__init__(self.n_paths, self.batch_size, shuffle, seed)\n",
    "        \n",
    "    def load_node_stimuli(self):\n",
    "        # Load the fractal images into memory\n",
    "        assert index.DIR_SCH_FRACTALS_RS.exists()\n",
    "        if self.mapping:\n",
    "            self.paths_data = [index.DIR_SCH_FRACTALS_RS / (name+'.npy')\n",
    "                               for name in self.mapping.values()]\n",
    "        else:\n",
    "            paths_data = list(index.DIR_SCH_FRACTALS_RS.iterdir())\n",
    "            np.random.shuffle(paths_data)\n",
    "            self.paths_data = paths_data[:5*self.n_pentagons]\n",
    "        self.array_data = np.array([np.load(str(path)) for path in self.paths_data])\n",
    "        \n",
    "    def iter_single_sample(self): \n",
    "        if self.mode == 'random':\n",
    "            iter_walk = sch.walk.walk_random(self.G, steps=self.max_steps)\n",
    "        elif self.mode == 'euclidean':\n",
    "            iter_walk = sch.walk.walk_euclidean(self.G)\n",
    "        elif self.mode == 'hamiltonian':\n",
    "            iter_walk = sch.walk.walk_hamiltonian(self.G)\n",
    "\n",
    "        for sample in iter_walk:\n",
    "            yield self.array_data[sample[0]], sample[0]\n",
    "        \n",
    "    def iter_batch_sample(self):\n",
    "        iter_batch = zip(*[self.iter_single_sample()\n",
    "                           for _ in range(self.batch_size)])\n",
    "        for batch in iter_batch:\n",
    "            data, nodes = zip(*batch)\n",
    "            yield data, nodes\n",
    "        \n",
    "    def iter_batch_dataset(self):   \n",
    "        for _ in range(self.n_paths):\n",
    "            data, nodes = zip(*list(self.iter_batch_sample()))\n",
    "            yield np.moveaxis(np.array(data), 0, 1), nodes\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self.iter_batch_dataset()\n",
    "    \n",
    "    def __getitem__(self, null):\n",
    "        data_iter = self.iter_batch_dataset()\n",
    "        data = next(data_iter)[0]\n",
    "        data2 = np.zeros(data.shape)\n",
    "        data2[:,:-1,:,:,:] = data[:,1:,:,:,:]\n",
    "        return data, data2\n",
    "    \n",
    "max_steps = 100\n",
    "epochs = 1\n",
    "batch_size = 14\n",
    "n_paths = 1\n",
    "\n",
    "iter_ds = ShapiroFractalsDataset(batch_size=batch_size, max_steps=max_steps, n_paths=n_paths,\n",
    "                                 mapping=mapping)\n",
    "for _ in range(epochs):\n",
    "    for i, batch in enumerate(iter_ds):\n",
    "        print(i, batch[0].shape)\n",
    "        if i > max_steps+5:\n",
    "            print('bad')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = fine_tune_data.shape[1]\n",
    "orig_weights_file = str(ks.WEIGHTS_DIR / 'tensorflow_weights/prednet_kitti_weights.hdf5')  # original t+1 weights\n",
    "orig_json_file = str(ks.WEIGHTS_DIR / 'prednet_kitti_model.json')\n",
    "\n",
    "fractals_weights_file = str(ks.WEIGHTS_DIR / 'tensorflow_weights/prednet_kitti_weights-fractals_finetuned.hdf5')  # where new weights will be saved\n",
    "fractals_json_file = str(ks.WEIGHTS_DIR / 'prednet_kitti_model-fractals_finetuned.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mapping as follows:\n",
      "{0: '16', 1: '97', 2: '62', 3: '94', 4: '19', 5: '32', 6: '40', 7: '15', 8: '10', 9: '86', 10: '51', 11: '33', 12: '30', 13: '29', 14: '61'}\n",
      "Epoch 1/200\n",
      "25/25 [==============================] - 143s 6s/step - loss: 13235.0278\n",
      "Epoch 2/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13209.7181\n",
      "Epoch 3/200\n",
      "25/25 [==============================] - 131s 5s/step - loss: 13245.2598\n",
      "Epoch 4/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13250.6855\n",
      "Epoch 5/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13265.5195\n",
      "Epoch 6/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13232.0273\n",
      "Epoch 7/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13221.7630\n",
      "Epoch 8/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13184.9684\n",
      "Epoch 9/200\n",
      "25/25 [==============================] - 133s 5s/step - loss: 13226.9379\n",
      "Epoch 10/200\n",
      "25/25 [==============================] - 131s 5s/step - loss: 13232.6815\n",
      "Epoch 11/200\n",
      "25/25 [==============================] - 131s 5s/step - loss: 13233.3284\n",
      "Epoch 12/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13224.3814\n",
      "Epoch 13/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13253.3164\n",
      "Epoch 14/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13219.4837\n",
      "Epoch 15/200\n",
      "25/25 [==============================] - 131s 5s/step - loss: 13204.1162\n",
      "Epoch 16/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13255.2393\n",
      "Epoch 17/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13217.4335\n",
      "Epoch 18/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13251.6574\n",
      "Epoch 19/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13226.6929\n",
      "Epoch 20/200\n",
      "25/25 [==============================] - 131s 5s/step - loss: 13181.9303\n",
      "Epoch 21/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13247.3884\n",
      "Epoch 22/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13253.5130\n",
      "Epoch 23/200\n",
      "25/25 [==============================] - 132s 5s/step - loss: 13225.5772\n",
      "Epoch 24/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13282.6991\n",
      "Epoch 25/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13238.7682\n",
      "Epoch 26/200\n",
      "25/25 [==============================] - 131s 5s/step - loss: 13238.9280\n",
      "Epoch 27/200\n",
      "25/25 [==============================] - 131s 5s/step - loss: 13265.2602\n",
      "Epoch 28/200\n",
      "25/25 [==============================] - 131s 5s/step - loss: 13244.0711\n",
      "Epoch 29/200\n",
      "25/25 [==============================] - 131s 5s/step - loss: 13198.9155\n",
      "Epoch 30/200\n",
      "25/25 [==============================] - 131s 5s/step - loss: 13227.0921\n",
      "Epoch 31/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13199.5578\n",
      "Epoch 32/200\n",
      "25/25 [==============================] - 131s 5s/step - loss: 13215.0012\n",
      "Epoch 33/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13219.2613\n",
      "Epoch 34/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13235.5132\n",
      "Epoch 35/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13250.8921\n",
      "Epoch 36/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13237.7037\n",
      "Epoch 37/200\n",
      "25/25 [==============================] - 131s 5s/step - loss: 13273.5977\n",
      "Epoch 38/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13262.9929\n",
      "Epoch 39/200\n",
      "25/25 [==============================] - 131s 5s/step - loss: 13220.3521\n",
      "Epoch 40/200\n",
      "25/25 [==============================] - 131s 5s/step - loss: 13238.5281\n",
      "Epoch 41/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13244.2594\n",
      "Epoch 42/200\n",
      "25/25 [==============================] - 131s 5s/step - loss: 13217.0971\n",
      "Epoch 43/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13219.6712\n",
      "Epoch 44/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13225.5079\n",
      "Epoch 45/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13250.9294\n",
      "Epoch 46/200\n",
      "25/25 [==============================] - 131s 5s/step - loss: 13230.7928\n",
      "Epoch 47/200\n",
      "25/25 [==============================] - 130s 5s/step - loss: 13267.4227\n",
      "Epoch 48/200\n",
      "25/25 [==============================] - 131s 5s/step - loss: 13223.7602\n",
      "Epoch 49/200\n",
      "17/25 [===================>..........] - ETA: 41s - loss: 13225.4796"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-89b979435a26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfractals_weights_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m history = model.fit_generator(train_generator, samples_per_epoch, nb_epoch, callbacks=callbacks,\n\u001b[0;32m---> 40\u001b[0;31m                 validation_data=None)\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/abdullah/envs/pn/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/abdullah/envs/pn/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/abdullah/envs/pn/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/abdullah/envs/pn/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/abdullah/envs/pn/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/media/data/conda/abdullah/envs/pn/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_model = True\n",
    "nb_epoch = 200\n",
    "batch_size = 4\n",
    "samples_per_epoch = None\n",
    "N_seq_val = 50  # number of sequences to use for validation\n",
    "max_steps = 100\n",
    "n_paths = 100\n",
    "nt = max_steps\n",
    "\n",
    "# Load t+1 model\n",
    "f = open(orig_json_file, 'r')\n",
    "json_string = f.read()\n",
    "f.close()\n",
    "orig_model = model_from_json(json_string, custom_objects = {'PredNet': pn.PredNet})\n",
    "orig_model.load_weights(orig_weights_file)\n",
    "\n",
    "layer_config = orig_model.layers[1].get_config()\n",
    "layer_config['output_mode'] = 'prediction'\n",
    "data_format = layer_config['data_format'] if 'data_format' in layer_config else layer_config['dim_ordering']\n",
    "prednet = pn.PredNet(weights=orig_model.layers[1].get_weights(), **layer_config)\n",
    "\n",
    "input_shape = list(orig_model.layers[0].batch_input_shape[1:])\n",
    "input_shape[0] = nt\n",
    "\n",
    "inputs = Input(input_shape)\n",
    "predictions = prednet(inputs)\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "train_generator = ShapiroFractalsDataset(batch_size=batch_size, max_steps=max_steps, n_paths=n_paths,\n",
    "                                    mapping=mapping)\n",
    "                                  \n",
    "lr_schedule = lambda epoch: 0.001 if epoch < 100 else 0.0001    # start with lr of 0.001 and then drop to 0.0001 after 75 epochs\n",
    "callbacks = [LearningRateScheduler(lr_schedule)]\n",
    "if save_model:\n",
    "    if not ks.WEIGHTS_DIR.exists(): \n",
    "        ks.WEIGHTS_DIR.mkdir(parents=True)\n",
    "    callbacks.append(ModelCheckpoint(filepath=fractals_weights_file, monitor='loss', save_best_only=True))\n",
    "history = model.fit_generator(train_generator, samples_per_epoch, nb_epoch, callbacks=callbacks,\n",
    "                validation_data=None)\n",
    "\n",
    "if save_model:\n",
    "    json_string = model.to_json()\n",
    "    with open(fractals_json_file, \"w\") as f:\n",
    "        f.write(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ended the run because it seems there isn't any learning happening, so no reason to run overnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "gcf = plt.gcf()\n",
    "gcf.set_size_inches(16,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = 36\n",
    "# Create testing model (to output predictions)\n",
    "layer_config = model.layers[1].get_config()\n",
    "X_Rs = []\n",
    "\n",
    "for i in range(len(layer_config['R_stack_sizes'])):\n",
    "\n",
    "    layer_config['output_mode'] = 'R' + str(i)\n",
    "    data_format = layer_config['data_format'] if 'data_format' in layer_config \\\n",
    "        else layer_config['dim_ordering']\n",
    "    test_prednet = pn.PredNet(weights=model.layers[1].get_weights(), \n",
    "                              **layer_config)\n",
    "\n",
    "    input_shape = list(model.layers[0].batch_input_shape[1:])\n",
    "    input_shape[0] = nt\n",
    "    inputs = Input(shape=tuple(input_shape))\n",
    "    R_outs = test_prednet(inputs)\n",
    "    test_model = Model(inputs=inputs, outputs=R_outs)\n",
    "\n",
    "    X_Rs.append(test_model.predict(test_data, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Rs[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, node in enumerate(test_walk_nodes):\n",
    "    print(i, node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borders = [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G, with_labels=True, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs_Rs = []\n",
    "for i in range(len(X_Rs)):\n",
    "    diffs_Rs.append(np.mean(np.diff(X_Rs[i][0], axis=0)**2, axis=(1,2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax_large = fig.add_subplot(111)\n",
    "\n",
    "for i, diff_R in enumerate(diffs_Rs):\n",
    "    ax = fig.add_subplot(11 + i + len(diffs_Rs)*100)\n",
    "    ax.plot(diff_R)\n",
    "    ax.set_ylabel(f'Layer {i+1}')\n",
    "    [ax.axes.axvline(b, ls=':') for b in borders]\n",
    "    if i != len(diffs_Rs)-1:\n",
    "        ax.axes.xaxis.set_ticks([])\n",
    "        \n",
    "ax_large.axes.xaxis.set_ticks([])\n",
    "ax_large.axes.yaxis.set_ticks([])\n",
    "ax_large.set_xlabel('Step')\n",
    "gcf = plt.gcf()\n",
    "gcf.set_size_inches(16,9)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
