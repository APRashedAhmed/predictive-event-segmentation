{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.3 Checking Normalization and Last Layer Reinitialization\n",
    "\n",
    "The images didn't go through any kind of normalization when sent through the model, and the last layer should be reinitialized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [watermark](https://github.com/rasbt/watermark) to see the state of the machine and environment that's running the notebook. To make sense of the options, take a look at the [usage](https://github.com/rasbt/watermark#usage) section of the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 05 2020 16:24:47 \n",
      "\n",
      "compiler   : GCC 7.3.0\n",
      "system     : Linux\n",
      "release    : 4.15.0-112-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n",
      "host name  : serrep5\n",
      "Git hash   : 51780b860c73fae4df359421a6c76fa3cb2ae7bb\n",
      "Git branch : master\n"
     ]
    }
   ],
   "source": [
    "# Load `watermark` extension\n",
    "%load_ext watermark\n",
    "# Display the status of the machine and other non-code related info\n",
    "%watermark -n -m -g -b -t -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [autoreload](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html) which will always reload modules marked with `%aimport`.\n",
    "\n",
    "This behavior can be inverted by running `autoreload 2` which will set everything to be auto-reloaded *except* for modules marked with `%aimport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `autoreload` extension\n",
    "%load_ext autoreload\n",
    "# Set autoreload behavior\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `matplotlib` in one of the more `jupyter`-friendly [rich-output modes](https://ipython.readthedocs.io/en/stable/interactive/plotting.html). Some options (that may or may not have worked) are `inline`, `notebook`, and `gtk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the matplotlib mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conda Env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: conda: command not found\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/data/conda/abdullah/envs/pn/lib/python3.7/site-packages/watermark/watermark.py:237: The name tf.VERSION is deprecated. Please use tf.version.VERSION instead.\n",
      "\n",
      "matplotlib 3.3.1\n",
      "numpy      1.19.1\n",
      "prednet    0+untagged.74.g57d9b43\n",
      "networkx   2.5\n",
      "prevseg    0+untagged.71.g51780b8.dirty\n",
      "PIL.Image  7.2.0\n",
      "tensorflow 1.15.0\n",
      "CPython 3.7.7\n",
      "IPython 7.17.0\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import Iterator\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "%aimport prevseg.index\n",
    "import prevseg.index as index\n",
    "%aimport prevseg.schapiro\n",
    "import prevseg.schapiro as sch\n",
    "from prevseg.schapiro import graph, walk\n",
    "%aimport prevseg.constants\n",
    "import prevseg.constants as const\n",
    "\n",
    "%aimport prednet.kitti_settings\n",
    "import prednet.kitti_settings as ks\n",
    "%aimport prednet.prednet_base\n",
    "import prednet.prednet_base as pn\n",
    "%aimport prednet.data_utils\n",
    "import prednet.data_utils as utils\n",
    "\n",
    "# Keep track of versions of everything\n",
    "%watermark -v -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Schapiro Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 14\n",
    "n_pentagons = 3\n",
    "max_steps = 100\n",
    "n_paths = 1\n",
    "source = 1\n",
    "\n",
    "paths_data = list(index.DIR_SCH_FRACTALS_RS.iterdir())\n",
    "np.random.shuffle(paths_data)\n",
    "paths_data = paths_data[:5*n_pentagons]\n",
    "array_data = np.array([np.load(str(path)) for path in paths_data])\n",
    "\n",
    "G = sch.graph.schapiro_graph(n_pentagons=n_pentagons)\n",
    "\n",
    "mapping = {node : path.stem for node, path in zip(range(len(G.nodes)), paths_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 100, 3, 128, 160)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def iter_single_sample(G, mode, max_steps, source=None): \n",
    "    if mode == 'random':\n",
    "        iter_walk = sch.walk.walk_random(G, steps=max_steps, source=source)\n",
    "    elif mode == 'euclidean':\n",
    "        iter_walk = sch.walk.walk_euclidean(G, source=source)\n",
    "    elif mode == 'hamiltonian':\n",
    "        iter_walk = sch.walk.walk_hamiltonian(G, source=source)\n",
    "    else:\n",
    "        raise ValueError('Invalid mode entered')\n",
    "\n",
    "    for sample in iter_walk:\n",
    "        yield array_data[sample[0]], sample[0]\n",
    "\n",
    "def iter_batch_sample(G, mode, max_steps, batch_size, source=None):\n",
    "    iter_batch = zip(*[iter_single_sample(G, mode, max_steps, source=source) \n",
    "                       for _ in range(batch_size)])\n",
    "    for batch in iter_batch:\n",
    "        data, nodes = zip(*batch)\n",
    "        yield data, nodes\n",
    "\n",
    "def iter_batch_dataset(G, mode, max_steps, batch_size, n_paths, source=None):       \n",
    "    for _ in range(n_paths):\n",
    "        data, nodes = zip(*list(iter_batch_sample(\n",
    "            G, mode, max_steps, batch_size, source=source)))\n",
    "        yield np.moveaxis(np.array(data), 0, 1), nodes\n",
    "\n",
    "fine_tune_data_iter = iter_batch_dataset(G, 'random', max_steps, batch_size, n_paths)\n",
    "fine_tune_data, fine_tune_nodes = next(fine_tune_data_iter)\n",
    "fine_tune_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_iter = iter_batch_dataset(G, 'euclidean', None, 1, n_paths, source=source)\n",
    "_, euclidean_nodes = next(euclidean_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 13,\n",
       " 14,\n",
       " 12,\n",
       " 13,\n",
       " 11,\n",
       " 12,\n",
       " 10,\n",
       " 11,\n",
       " 14,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_nodes = list(np.array(euclidean_nodes).reshape(30))\n",
    "euclidean_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 36, 3, 128, 160)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inlcude some non-important steps the start to allow the hidden state to adapt\n",
    "initial_padding = [1,0,1,0,1,0]\n",
    "test_walk_nodes = initial_padding + euclidean_nodes\n",
    "\n",
    "test_data = np.expand_dims(np.array([array_data[n] for n in test_walk_nodes]),0)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sequence Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mapping as follows:\n",
      "{0: '40', 1: '15', 2: '91', 3: '29', 4: '22', 5: '99', 6: '79', 7: '19', 8: '45', 9: '85', 10: '56', 11: '24', 12: '71', 13: '77', 14: '21'}\n",
      "0 (14, 100, 3, 128, 160)\n"
     ]
    }
   ],
   "source": [
    "class ShapiroFractalsDataset(Iterator):\n",
    "    modes = set(('random', 'euclidean', 'hamiltonian'))\n",
    "    def __init__(self, batch_size=32, n_pentagons=3, max_steps=128, n_paths=128,\n",
    "                 mapping=None, mode='random', debug=False, seed=None, shuffle=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.n_pentagons = n_pentagons\n",
    "        self.max_steps = max_steps\n",
    "        self.n_paths = n_paths\n",
    "        self.mapping = mapping\n",
    "        self.mode = mode\n",
    "        self.debug = debug\n",
    "        assert self.mode in self.modes\n",
    "        \n",
    "        self.G = graph.schapiro_graph(n_pentagons=n_pentagons)\n",
    "        \n",
    "        self.load_node_stimuli()\n",
    "        \n",
    "        self.mapping = {node : path.stem\n",
    "                        for node, path in zip(range(len(self.G.nodes)),\n",
    "                                              self.paths_data)}\n",
    "        print(f'Created mapping as follows:\\n{self.mapping}')\n",
    "        \n",
    "        if self.debug:\n",
    "            self.sample_transform = lambda sample : sample\n",
    "        else:\n",
    "            self.sample_transform = lambda sample : self.array_data[sample]\n",
    "        super().__init__(self.n_paths, self.batch_size, shuffle, seed)\n",
    "        \n",
    "    def load_node_stimuli(self):\n",
    "        # Load the fractal images into memory\n",
    "        assert index.DIR_SCH_FRACTALS_RS.exists()\n",
    "        if self.mapping:\n",
    "            self.paths_data = [index.DIR_SCH_FRACTALS_RS / (name+'.npy')\n",
    "                               for name in self.mapping.values()]\n",
    "        else:\n",
    "            paths_data = list(index.DIR_SCH_FRACTALS_RS.iterdir())\n",
    "            np.random.shuffle(paths_data)\n",
    "            self.paths_data = paths_data[:5*self.n_pentagons]\n",
    "        self.array_data = np.array([np.load(str(path)) for path in self.paths_data])\n",
    "        self.array_data = self.array_data - np.array(const.IMAGENET_NORM_MEAN).reshape((1,3,1,1))\n",
    "        self.array_data = self.array_data / np.array(const.IMAGENET_NORM_STD).reshape((1,3,1,1))\n",
    "        \n",
    "    def iter_single_sample(self): \n",
    "        if self.mode == 'random':\n",
    "            iter_walk = sch.walk.walk_random(self.G, steps=self.max_steps)\n",
    "        elif self.mode == 'euclidean':\n",
    "            iter_walk = sch.walk.walk_euclidean(self.G)\n",
    "        elif self.mode == 'hamiltonian':\n",
    "            iter_walk = sch.walk.walk_hamiltonian(self.G)\n",
    "\n",
    "        for sample in iter_walk:\n",
    "            yield self.array_data[sample[0]], sample[0]\n",
    "        \n",
    "    def iter_batch_sample(self):\n",
    "        iter_batch = zip(*[self.iter_single_sample()\n",
    "                           for _ in range(self.batch_size)])\n",
    "        for batch in iter_batch:\n",
    "            data, nodes = zip(*batch)\n",
    "            yield data, nodes\n",
    "        \n",
    "    def iter_batch_dataset(self):   \n",
    "        for _ in range(self.n_paths):\n",
    "            data, nodes = zip(*list(self.iter_batch_sample()))\n",
    "            yield np.moveaxis(np.array(data), 0, 1), nodes\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self.iter_batch_dataset()\n",
    "    \n",
    "    def __getitem__(self, null):\n",
    "        data_iter = self.iter_batch_dataset()\n",
    "        data = next(data_iter)[0]\n",
    "        data2 = np.zeros(data.shape)\n",
    "        data2[:,:-1,:,:,:] = data[:,1:,:,:,:]\n",
    "        return data, data2\n",
    "    \n",
    "max_steps = 100\n",
    "epochs = 1\n",
    "batch_size = 14\n",
    "n_paths = 1\n",
    "\n",
    "iter_ds = ShapiroFractalsDataset(batch_size=batch_size, max_steps=max_steps, n_paths=n_paths,\n",
    "                                 mapping=mapping)\n",
    "for _ in range(epochs):\n",
    "    for i, batch in enumerate(iter_ds):\n",
    "        print(i, batch[0].shape)\n",
    "        if i == max_steps:\n",
    "            print('bad')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 3, 128, 160)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_ds.array_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinitializing the Readout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = fine_tune_data.shape[1]\n",
    "orig_weights_file = str(ks.WEIGHTS_DIR / 'tensorflow_weights/prednet_kitti_weights.hdf5')  # original t+1 weights\n",
    "orig_json_file = str(ks.WEIGHTS_DIR / 'prednet_kitti_model.json')\n",
    "\n",
    "fractals_weights_file = str(ks.WEIGHTS_DIR / 'tensorflow_weights/prednet_kitti_weights-fractals_finetuned.hdf5')  # where new weights will be saved\n",
    "fractals_json_file = str(ks.WEIGHTS_DIR / 'prednet_kitti_model-fractals_finetuned.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = True\n",
    "nb_epoch = 600\n",
    "batch_size = 3\n",
    "samples_per_epoch = None\n",
    "N_seq_val = 50  # number of sequences to use for validation\n",
    "max_steps = 50\n",
    "n_paths = 60\n",
    "nt = max_steps\n",
    "\n",
    "# Load t+1 model\n",
    "f = open(orig_json_file, 'r')\n",
    "json_string = f.read()\n",
    "f.close()\n",
    "orig_model = model_from_json(json_string, custom_objects = {'PredNet': pn.PredNet})\n",
    "orig_model.load_weights(orig_weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6,\n",
       "  <tf.Variable 'prednet_1_4/layer_ahat_0/kernel:0' shape=(3, 3, 3, 3) dtype=float32>),\n",
       " (7, <tf.Variable 'prednet_1_4/layer_ahat_0/bias:0' shape=(3,) dtype=float32>)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readout_weights = [(i, w) for i, w in \n",
    "                   enumerate(orig_model.layers[1].trainable_weights) \n",
    "                   if 'ahat_0' in w.name]\n",
    "readout_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_weights = orig_model.layers[1].get_weights()\n",
    "assert all([np.all(w1 == w2) for w1, w2 in\n",
    "            zip(orig_model.layers[1].get_weights(), orig_weights)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in readout_weights:\n",
    "    new_w = np.random.rand(*w.shape.as_list())*np.sqrt(1/(sum(w.shape.as_list())))\n",
    "    assert new_w.shape == w.shape\n",
    "    orig_weights[i] = new_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not all([np.all(w1 == w2) for w1, w2 in \n",
    "                zip(orig_model.layers[1].get_weights(), orig_weights)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_config = orig_model.layers[1].get_config()\n",
    "layer_config['output_mode'] = 'prediction'\n",
    "data_format = layer_config['data_format'] if 'data_format' in layer_config else layer_config['dim_ordering']\n",
    "prednet = pn.PredNet(weights=orig_weights, **layer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://erikbrorson.github.io/2018/04/30/Adam-with-learning-rate-multipliers/\n",
    "from keras.legacy import interfaces\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Optimizer\n",
    "\n",
    "class Adam_lr_mult(Optimizer):\n",
    "    \"\"\"Adam optimizer.\n",
    "    Adam optimizer, with learning rate multipliers built on Keras implementation\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
    "            algorithm from the paper \"On the Convergence of Adam and\n",
    "            Beyond\".\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n",
    "        - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n",
    "        \n",
    "    AUTHOR: Erik Brorson\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., amsgrad=False,\n",
    "                 multipliers=None, debug_verbose=False,**kwargs):\n",
    "        super(Adam_lr_mult, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.learning_rate = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsgrad = amsgrad\n",
    "        self.multipliers = multipliers\n",
    "        self.debug_verbose = debug_verbose\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.learning_rate\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                  K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "\n",
    "            # Learning rate multipliers\n",
    "            if self.multipliers:\n",
    "                multiplier = [mult for mult in self.multipliers if mult in p.name]\n",
    "            else:\n",
    "                multiplier = None\n",
    "            if multiplier:\n",
    "                new_lr_t = lr_t * self.multipliers[multiplier[0]]\n",
    "                if self.debug_verbose:\n",
    "                    print('Setting {} to learning rate {}'.format(multiplier[0], new_lr_t))\n",
    "                    print(K.get_value(new_lr_t))\n",
    "            else:\n",
    "                new_lr_t = lr_t\n",
    "                if self.debug_verbose:\n",
    "                    print('No change in learning rate {}'.format(p.name))\n",
    "                    print(K.get_value(new_lr_t))\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                p_t = p - new_lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                p_t = p - new_lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.learning_rate)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'amsgrad': self.amsgrad,\n",
    "                  'multipliers':self.multipliers}\n",
    "        base_config = super(Adam_lr_mult, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_multipliers =  {w.name : 0.001 for w in orig_model.layers[1].trainable_weights}\n",
    "for i, w in readout_weights:\n",
    "    learning_rate_multipliers[w.name] = 1\n",
    "\n",
    "adam_lr_mult = Adam_lr_mult(multipliers=learning_rate_multipliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = list(orig_model.layers[0].batch_input_shape[1:])\n",
    "input_shape[0] = nt\n",
    "\n",
    "inputs = Input(input_shape)\n",
    "predictions = prednet(inputs)\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(loss='mse', optimizer=adam_lr_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mapping as follows:\n",
      "{0: '40', 1: '15', 2: '91', 3: '29', 4: '22', 5: '99', 6: '79', 7: '19', 8: '45', 9: '85', 10: '56', 11: '24', 12: '71', 13: '77', 14: '21'}\n",
      "Epoch 1/600\n",
      "20/20 [==============================] - 38s 2s/step - loss: 253861.9875\n",
      "Epoch 2/600\n",
      "10/20 [==============>...............] - ETA: 14s - loss: 255094.6594"
     ]
    }
   ],
   "source": [
    "train_generator = ShapiroFractalsDataset(batch_size=batch_size, max_steps=max_steps, n_paths=n_paths,\n",
    "                                    mapping=mapping)\n",
    "\n",
    "lr_schedule = lambda epoch: 0.001 if epoch < nb_epoch/2 else 0.0001    # start with lr of 0.001 and then drop to 0.0001 after 75 epochs\n",
    "callbacks = [LearningRateScheduler(lr_schedule)]\n",
    "if save_model:\n",
    "    if not ks.WEIGHTS_DIR.exists(): \n",
    "        ks.WEIGHTS_DIR.mkdir(parents=True)\n",
    "    callbacks.append(ModelCheckpoint(filepath=fractals_weights_file, monitor='loss', save_best_only=True))\n",
    "history = model.fit_generator(train_generator, samples_per_epoch, nb_epoch, callbacks=callbacks,\n",
    "                validation_data=None)\n",
    "\n",
    "if save_model:\n",
    "    json_string = model.to_json()\n",
    "    with open(fractals_json_file, \"w\") as f:\n",
    "        f.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "gcf = plt.gcf()\n",
    "gcf.set_size_inches(16,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = 36\n",
    "# Create testing model (to output predictions)\n",
    "layer_config = model.layers[1].get_config()\n",
    "X_Rs = []\n",
    "\n",
    "for i in range(len(layer_config['R_stack_sizes'])):\n",
    "\n",
    "    layer_config['output_mode'] = 'R' + str(i)\n",
    "    data_format = layer_config['data_format'] if 'data_format' in layer_config \\\n",
    "        else layer_config['dim_ordering']\n",
    "    test_prednet = pn.PredNet(weights=model.layers[1].get_weights(), \n",
    "                              **layer_config)\n",
    "\n",
    "    input_shape = list(model.layers[0].batch_input_shape[1:])\n",
    "    input_shape[0] = nt\n",
    "    inputs = Input(shape=tuple(input_shape))\n",
    "    R_outs = test_prednet(inputs)\n",
    "    test_model = Model(inputs=inputs, outputs=R_outs)\n",
    "\n",
    "    X_Rs.append(test_model.predict(test_data, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Rs[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, node in enumerate(test_walk_nodes):\n",
    "    print(i, node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borders = [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G, with_labels=True, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs_Rs = []\n",
    "for i in range(len(X_Rs)):\n",
    "    diffs_Rs.append(np.mean(np.diff(X_Rs[i][0], axis=0)**2, axis=(1,2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax_large = fig.add_subplot(111)\n",
    "\n",
    "for i, diff_R in enumerate(diffs_Rs):\n",
    "    ax = fig.add_subplot(11 + i + len(diffs_Rs)*100)\n",
    "    ax.plot(diff_R)\n",
    "    ax.set_ylabel(f'Layer {i+1}')\n",
    "    [ax.axes.axvline(b, ls=':') for b in borders]\n",
    "    if i != len(diffs_Rs)-1:\n",
    "        ax.axes.xaxis.set_ticks([])\n",
    "        \n",
    "ax_large.axes.xaxis.set_ticks([])\n",
    "ax_large.axes.yaxis.set_ticks([])\n",
    "ax_large.set_xlabel('Step')\n",
    "gcf = plt.gcf()\n",
    "gcf.set_size_inches(16,9)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
