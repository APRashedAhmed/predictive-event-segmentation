{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.3 Checking Normalization and Last Layer Reinitialization\n",
    "\n",
    "The images didn't go through any kind of normalization when sent through the model, and the last layer should be reinitialized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [watermark](https://github.com/rasbt/watermark) to see the state of the machine and environment that's running the notebook. To make sense of the options, take a look at the [usage](https://github.com/rasbt/watermark#usage) section of the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 05 2020 14:05:49 \n",
      "\n",
      "compiler   : GCC 7.5.0\n",
      "system     : Linux\n",
      "release    : 5.4.0-45-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "host name  : apra-x3\n",
      "Git hash   : f4d618200f8b9a5ff35e9f91319abe7a56b675bc\n",
      "Git branch : master\n"
     ]
    }
   ],
   "source": [
    "# Load `watermark` extension\n",
    "%load_ext watermark\n",
    "# Display the status of the machine and other non-code related info\n",
    "%watermark -n -m -g -b -t -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [autoreload](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html) which will always reload modules marked with `%aimport`.\n",
    "\n",
    "This behavior can be inverted by running `autoreload 2` which will set everything to be auto-reloaded *except* for modules marked with `%aimport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `autoreload` extension\n",
    "%load_ext autoreload\n",
    "# Set autoreload behavior\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `matplotlib` in one of the more `jupyter`-friendly [rich-output modes](https://ipython.readthedocs.io/en/stable/interactive/plotting.html). Some options (that may or may not have worked) are `inline`, `notebook`, and `gtk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the matplotlib mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conda Env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/apra/miniconda3/envs/prednet:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_libgcc_mutex             0.1                 conda_forge    conda-forge\n",
      "_openmp_mutex             4.5                       1_gnu    conda-forge\n",
      "_tflow_select             2.1.0                       gpu    anaconda\n",
      "absl-py                   0.9.0            py37hc8dfbb8_1    conda-forge\n",
      "argon2-cffi               20.1.0                   pypi_0    pypi\n",
      "astor                     0.8.1              pyh9f0ad1d_0    conda-forge\n",
      "async-generator           1.10                     pypi_0    pypi\n",
      "attrs                     20.1.0             pyh9f0ad1d_0    conda-forge\n",
      "babel                     2.8.0                    pypi_0    pypi\n",
      "backcall                  0.2.0                    pypi_0    pypi\n",
      "beautifulsoup4            4.9.1                      py_1    conda-forge\n",
      "bleach                    3.1.5                    pypi_0    pypi\n",
      "brotlipy                  0.7.0           py37h8f50634_1000    conda-forge\n",
      "c-ares                    1.16.1               h516909a_2    conda-forge\n",
      "ca-certificates           2020.6.20            hecda079_0    conda-forge\n",
      "certifi                   2020.6.20        py37hc8dfbb8_0    conda-forge\n",
      "cffi                      1.14.1           py37h2b28604_0    conda-forge\n",
      "chardet                   3.0.4           py37hc8dfbb8_1006    conda-forge\n",
      "codecov                   2.1.9              pyh9f0ad1d_0    conda-forge\n",
      "coloredlogs               14.0             py37hc8dfbb8_1    conda-forge\n",
      "coverage                  5.2.1            py37h8f50634_0    conda-forge\n",
      "cryptography              3.0              py37hb09aad4_0    conda-forge\n",
      "cudatoolkit               10.0.130                      0    anaconda\n",
      "cudnn                     7.6.5                cuda10.0_0    anaconda\n",
      "cupti                     10.0.130                      0    anaconda\n",
      "cycler                    0.10.0                     py_2    conda-forge\n",
      "decorator                 4.4.2                    pypi_0    pypi\n",
      "defusedxml                0.7.0rc1                 pypi_0    pypi\n",
      "dill                      0.3.2                    pypi_0    pypi\n",
      "entrypoints               0.3                      pypi_0    pypi\n",
      "ffmpeg-python             0.2.0                    pypi_0    pypi\n",
      "flake8                    3.8.3                      py_1    conda-forge\n",
      "freetype                  2.10.2               he06d7ca_0    conda-forge\n",
      "future                    0.18.2                   pypi_0    pypi\n",
      "gast                      0.2.2                      py_0    conda-forge\n",
      "google-pasta              0.2.0              pyh8c360ce_0    conda-forge\n",
      "grpcio                    1.31.0           py37hb0870dc_0    conda-forge\n",
      "h5py                      2.10.0          nompi_py37h90cd8ad_104    conda-forge\n",
      "hdf5                      1.10.6          nompi_h3c11f04_101    conda-forge\n",
      "hickle                    2.1.0                    pypi_0    pypi\n",
      "humanfriendly             8.2              py37hc8dfbb8_0    conda-forge\n",
      "idna                      2.10               pyh9f0ad1d_0    conda-forge\n",
      "imageio                   2.9.0                      py_0    conda-forge\n",
      "imageio-ffmpeg            0.4.2                    pypi_0    pypi\n",
      "importlib-metadata        1.7.0            py37hc8dfbb8_0    conda-forge\n",
      "importlib_metadata        1.7.0                         0    conda-forge\n",
      "iniconfig                 1.0.1              pyh9f0ad1d_0    conda-forge\n",
      "ipykernel                 5.3.4                    pypi_0    pypi\n",
      "ipython                   7.17.0                   pypi_0    pypi\n",
      "ipython-genutils          0.2.0                    pypi_0    pypi\n",
      "ipython_genutils          0.2.0                    py37_0    defaults\n",
      "jedi                      0.17.2                   pypi_0    pypi\n",
      "jinja2                    3.0.0a1                  pypi_0    pypi\n",
      "jpeg                      9d                   h516909a_0    conda-forge\n",
      "json5                     0.9.5                    pypi_0    pypi\n",
      "jsonschema                3.2.0                    pypi_0    pypi\n",
      "jupyter-client            6.1.6                    pypi_0    pypi\n",
      "jupyter-core              4.6.3                    pypi_0    pypi\n",
      "jupyter-server            1.0.0rc7                 pypi_0    pypi\n",
      "jupyter_client            6.1.6                      py_0    defaults\n",
      "jupyter_core              4.6.3                    py37_0    defaults\n",
      "jupyterlab                3.0.0a14                 pypi_0    pypi\n",
      "jupyterlab-pygments       0.1.1                    pypi_0    pypi\n",
      "jupyterlab-server         2.0.0b2                  pypi_0    pypi\n",
      "jupyterlab_server         1.2.0                      py_0    defaults\n",
      "keras                     2.3.1                    py37_0    conda-forge\n",
      "keras-applications        1.0.8                      py_1    conda-forge\n",
      "keras-preprocessing       1.1.0                      py_0    conda-forge\n",
      "kiwisolver                1.2.0            py37h99015e2_0    conda-forge\n",
      "lcms2                     2.11                 hbd6801e_0    conda-forge\n",
      "ld_impl_linux-64          2.34                 hc38a660_9    conda-forge\n",
      "libblas                   3.8.0               17_openblas    conda-forge\n",
      "libcblas                  3.8.0               17_openblas    conda-forge\n",
      "libffi                    3.2.1             he1b5a44_1007    conda-forge\n",
      "libgcc-ng                 9.3.0               h24d8f2e_16    conda-forge\n",
      "libgfortran-ng            7.5.0               hdf63c60_16    conda-forge\n",
      "libgomp                   9.3.0               h24d8f2e_16    conda-forge\n",
      "libgpuarray               0.7.6             h14c3975_1003    conda-forge\n",
      "liblapack                 3.8.0               17_openblas    conda-forge\n",
      "libopenblas               0.3.10          pthreads_hb3c22a3_4    conda-forge\n",
      "libpng                    1.6.37               hed695b0_2    conda-forge\n",
      "libprotobuf               3.13.0               h8b12597_0    conda-forge\n",
      "libsodium                 1.0.18               h7b6447c_0    defaults\n",
      "libstdcxx-ng              9.3.0               hdf63c60_16    conda-forge\n",
      "libtiff                   4.1.0                hc7e4089_6    conda-forge\n",
      "libwebp-base              1.1.0                h516909a_3    conda-forge\n",
      "lz4-c                     1.9.2                he1b5a44_3    conda-forge\n",
      "mako                      1.1.3              pyh9f0ad1d_0    conda-forge\n",
      "markdown                  3.2.2                      py_0    conda-forge\n",
      "markupsafe                1.1.1            py37h8f50634_1    conda-forge\n",
      "matplotlib                3.3.1                         1    conda-forge\n",
      "matplotlib-base           3.3.1            py37hd478181_1    conda-forge\n",
      "mccabe                    0.6.1                      py_1    conda-forge\n",
      "mistune                   0.8.4                    pypi_0    pypi\n",
      "more-itertools            8.4.0                      py_0    conda-forge\n",
      "nbclassic                 0.2.0rc4                 pypi_0    pypi\n",
      "nbclient                  0.4.1                    pypi_0    pypi\n",
      "nbconvert                 6.0.0b7                  pypi_0    pypi\n",
      "nbformat                  5.0.7                    pypi_0    pypi\n",
      "ncurses                   6.2                  he1b5a44_1    conda-forge\n",
      "nest-asyncio              1.4.0                    pypi_0    pypi\n",
      "networkx                  2.5                      pypi_0    pypi\n",
      "notebook                  6.1.3                    pypi_0    pypi\n",
      "numpy                     1.19.1           py37h7ea13bd_2    conda-forge\n",
      "olefile                   0.46                       py_0    conda-forge\n",
      "openssl                   1.1.1g               h516909a_1    conda-forge\n",
      "opt-einsum                3.3.0                    pypi_0    pypi\n",
      "opt_einsum                3.3.0                      py_0    conda-forge\n",
      "packaging                 20.4               pyh9f0ad1d_0    conda-forge\n",
      "pandoc                    2.10.1                        0    defaults\n",
      "pandocfilters             1.4.2                    py37_1    defaults\n",
      "parso                     0.7.1                    pypi_0    pypi\n",
      "pexpect                   4.8.0                    pypi_0    pypi\n",
      "pickleshare               0.7.5                    pypi_0    pypi\n",
      "pillow                    7.2.0            py37h718be6c_1    conda-forge\n",
      "pip                       20.2.2                     py_0    conda-forge\n",
      "pluggy                    0.13.1           py37hc8dfbb8_2    conda-forge\n",
      "predictive-event-segmentation 0+untagged.51.gcb26ef6.dirty           dev_0    <develop>\n",
      "prednet                   0+untagged.69.g843b452.dirty           dev_0    <develop>\n",
      "prometheus-client         0.8.0                    pypi_0    pypi\n",
      "prometheus_client         0.8.0                      py_0    defaults\n",
      "prompt-toolkit            3.0.6                    pypi_0    pypi\n",
      "protobuf                  3.13.0           py37h3340039_0    conda-forge\n",
      "ptyprocess                0.6.0                    pypi_0    pypi\n",
      "py                        1.9.0              pyh9f0ad1d_0    conda-forge\n",
      "pycodestyle               2.6.0              pyh9f0ad1d_0    conda-forge\n",
      "pycparser                 2.20               pyh9f0ad1d_2    conda-forge\n",
      "pyflakes                  2.2.0              pyh9f0ad1d_0    conda-forge\n",
      "pygments                  2.6.1                    pypi_0    pypi\n",
      "pygpu                     0.7.6           py37h03ebfcd_1001    conda-forge\n",
      "pyopenssl                 19.1.0                     py_1    conda-forge\n",
      "pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge\n",
      "pyrsistent                0.16.0                   pypi_0    pypi\n",
      "pysocks                   1.7.1            py37hc8dfbb8_1    conda-forge\n",
      "pytest                    6.0.1            py37hc8dfbb8_0    conda-forge\n",
      "python                    3.7.8           h6f2ec95_1_cpython    conda-forge\n",
      "python-dateutil           2.8.1                      py_0    conda-forge\n",
      "python_abi                3.7                     1_cp37m    conda-forge\n",
      "pytz                      2020.1                   pypi_0    pypi\n",
      "pyyaml                    5.3.1            py37h8f50634_0    conda-forge\n",
      "pyzmq                     19.0.2                   pypi_0    pypi\n",
      "readline                  8.0                  he28a2e2_2    conda-forge\n",
      "requests                  2.24.0             pyh9f0ad1d_0    conda-forge\n",
      "scikit-video              1.1.11                   pypi_0    pypi\n",
      "scipy                     1.5.2            py37hb14ef9d_0    conda-forge\n",
      "send2trash                1.6.0b1                  pypi_0    pypi\n",
      "setuptools                49.6.0           py37hc8dfbb8_0    conda-forge\n",
      "six                       1.15.0             pyh9f0ad1d_0    conda-forge\n",
      "soupsieve                 2.0.1                      py_1    conda-forge\n",
      "sqlite                    3.33.0               h4cf870e_0    conda-forge\n",
      "tensorboard               1.15.0                   py37_0    conda-forge\n",
      "tensorflow                1.15.0          gpu_py37h0f0df58_0    anaconda\n",
      "tensorflow-base           1.15.0          gpu_py37h9dcbed7_0    anaconda\n",
      "tensorflow-estimator      1.15.1             pyh2649769_0    anaconda\n",
      "tensorflow-gpu            1.15.3                   pypi_0    pypi\n",
      "termcolor                 1.1.0                      py_2    conda-forge\n",
      "terminado                 0.8.3                    pypi_0    pypi\n",
      "testpath                  0.4.4                    pypi_0    pypi\n",
      "theano                    1.0.3            py37hfc679d8_1    conda-forge\n",
      "tk                        8.6.10               hed695b0_0    conda-forge\n",
      "toml                      0.10.1             pyh9f0ad1d_0    conda-forge\n",
      "toolchain                 2.4.0                         0    anaconda\n",
      "toolchain_c_linux-64      2.4.0                         0    anaconda\n",
      "toolchain_cxx_linux-64    2.4.0                         0    anaconda\n",
      "tornado                   6.0.4            py37h8f50634_1    conda-forge\n",
      "traitlets                 5.0.0rc2                 pypi_0    pypi\n",
      "urllib3                   1.25.10                    py_0    conda-forge\n",
      "versioneer                0.18                       py_1    conda-forge\n",
      "watermark                 2.0.2                      py_0    conda-forge\n",
      "wcwidth                   0.2.5                    pypi_0    pypi\n",
      "webencodings              0.5.1                    pypi_0    pypi\n",
      "werkzeug                  0.16.1                     py_0    conda-forge\n",
      "wheel                     0.35.1                   pypi_0    pypi\n",
      "wrapt                     1.12.1           py37h8f50634_1    conda-forge\n",
      "xz                        5.2.5                h516909a_1    conda-forge\n",
      "yaml                      0.2.5                h516909a_0    conda-forge\n",
      "zeromq                    4.3.2                he6710b0_2    defaults\n",
      "zipp                      3.1.0                      py_0    conda-forge\n",
      "zlib                      1.2.11            h516909a_1007    conda-forge\n",
      "zstd                      1.4.5                h6597ccf_2    conda-forge\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/apra/miniconda3/envs/prednet/lib/python3.7/site-packages/watermark/watermark.py:237: The name tf.VERSION is deprecated. Please use tf.version.VERSION instead.\n",
      "\n",
      "matplotlib 3.3.1\n",
      "PIL.Image  7.2.0\n",
      "prednet    0+untagged.74.g57d9b43\n",
      "prevseg    0+untagged.69.gf4d6182.dirty\n",
      "tensorflow 1.15.0\n",
      "networkx   2.5\n",
      "numpy      1.19.1\n",
      "CPython 3.7.8\n",
      "IPython 7.17.0\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import Iterator\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "%aimport prevseg.index\n",
    "import prevseg.index as index\n",
    "%aimport prevseg.schapiro\n",
    "import prevseg.schapiro as sch\n",
    "from prevseg.schapiro import graph, walk\n",
    "%aimport prevseg.constants\n",
    "import prevseg.constants as const\n",
    "\n",
    "%aimport prednet.kitti_settings\n",
    "import prednet.kitti_settings as ks\n",
    "%aimport prednet.prednet_base\n",
    "import prednet.prednet_base as pn\n",
    "%aimport prednet.data_utils\n",
    "import prednet.data_utils as utils\n",
    "\n",
    "# Keep track of versions of everything\n",
    "%watermark -v -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Schapiro Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 14\n",
    "n_pentagons = 3\n",
    "max_steps = 100\n",
    "n_paths = 1\n",
    "source = 1\n",
    "\n",
    "paths_data = list(index.DIR_SCH_FRACTALS_RS.iterdir())\n",
    "np.random.shuffle(paths_data)\n",
    "paths_data = paths_data[:5*n_pentagons]\n",
    "array_data = np.array([np.load(str(path)) for path in paths_data])\n",
    "\n",
    "G = sch.graph.schapiro_graph(n_pentagons=n_pentagons)\n",
    "\n",
    "mapping = {node : path.stem for node, path in zip(range(len(G.nodes)), paths_data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 100, 3, 128, 160)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def iter_single_sample(G, mode, max_steps, source=None): \n",
    "    if mode == 'random':\n",
    "        iter_walk = sch.walk.walk_random(G, steps=max_steps, source=source)\n",
    "    elif mode == 'euclidean':\n",
    "        iter_walk = sch.walk.walk_euclidean(G, source=source)\n",
    "    elif mode == 'hamiltonian':\n",
    "        iter_walk = sch.walk.walk_hamiltonian(G, source=source)\n",
    "    else:\n",
    "        raise ValueError('Invalid mode entered')\n",
    "\n",
    "    for sample in iter_walk:\n",
    "        yield array_data[sample[0]], sample[0]\n",
    "\n",
    "def iter_batch_sample(G, mode, max_steps, batch_size, source=None):\n",
    "    iter_batch = zip(*[iter_single_sample(G, mode, max_steps, source=source) \n",
    "                       for _ in range(batch_size)])\n",
    "    for batch in iter_batch:\n",
    "        data, nodes = zip(*batch)\n",
    "        yield data, nodes\n",
    "\n",
    "def iter_batch_dataset(G, mode, max_steps, batch_size, n_paths, source=None):       \n",
    "    for _ in range(n_paths):\n",
    "        data, nodes = zip(*list(iter_batch_sample(\n",
    "            G, mode, max_steps, batch_size, source=source)))\n",
    "        yield np.moveaxis(np.array(data), 0, 1), nodes\n",
    "\n",
    "fine_tune_data_iter = iter_batch_dataset(G, 'random', max_steps, batch_size, n_paths)\n",
    "fine_tune_data, fine_tune_nodes = next(fine_tune_data_iter)\n",
    "fine_tune_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_iter = iter_batch_dataset(G, 'euclidean', None, 1, n_paths, source=source)\n",
    "_, euclidean_nodes = next(euclidean_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 13,\n",
       " 14,\n",
       " 12,\n",
       " 13,\n",
       " 11,\n",
       " 12,\n",
       " 10,\n",
       " 11,\n",
       " 14,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_nodes = list(np.array(euclidean_nodes).reshape(30))\n",
    "euclidean_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 36, 3, 128, 160)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inlcude some non-important steps the start to allow the hidden state to adapt\n",
    "initial_padding = [1,0,1,0,1,0]\n",
    "test_walk_nodes = initial_padding + euclidean_nodes\n",
    "\n",
    "test_data = np.expand_dims(np.array([array_data[n] for n in test_walk_nodes]),0)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sequence Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mapping as follows:\n",
      "{0: '42', 1: '81', 2: '64', 3: '7', 4: '61', 5: '35', 6: '18', 7: '83', 8: '60', 9: '3', 10: '53', 11: '10', 12: '22', 13: '8', 14: '91'}\n",
      "0 (14, 100, 3, 128, 160)\n"
     ]
    }
   ],
   "source": [
    "class ShapiroFractalsDataset(Iterator):\n",
    "    modes = set(('random', 'euclidean', 'hamiltonian'))\n",
    "    def __init__(self, batch_size=32, n_pentagons=3, max_steps=128, n_paths=128,\n",
    "                 mapping=None, mode='random', debug=False, seed=None, shuffle=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.n_pentagons = n_pentagons\n",
    "        self.max_steps = max_steps\n",
    "        self.n_paths = n_paths\n",
    "        self.mapping = mapping\n",
    "        self.mode = mode\n",
    "        self.debug = debug\n",
    "        assert self.mode in self.modes\n",
    "        \n",
    "        self.G = graph.schapiro_graph(n_pentagons=n_pentagons)\n",
    "        \n",
    "        self.load_node_stimuli()\n",
    "        \n",
    "        self.mapping = {node : path.stem\n",
    "                        for node, path in zip(range(len(self.G.nodes)),\n",
    "                                              self.paths_data)}\n",
    "        print(f'Created mapping as follows:\\n{self.mapping}')\n",
    "        \n",
    "        if self.debug:\n",
    "            self.sample_transform = lambda sample : sample\n",
    "        else:\n",
    "            self.sample_transform = lambda sample : self.array_data[sample]\n",
    "        super().__init__(self.n_paths, self.batch_size, shuffle, seed)\n",
    "        \n",
    "    def load_node_stimuli(self):\n",
    "        # Load the fractal images into memory\n",
    "        assert index.DIR_SCH_FRACTALS_RS.exists()\n",
    "        if self.mapping:\n",
    "            self.paths_data = [index.DIR_SCH_FRACTALS_RS / (name+'.npy')\n",
    "                               for name in self.mapping.values()]\n",
    "        else:\n",
    "            paths_data = list(index.DIR_SCH_FRACTALS_RS.iterdir())\n",
    "            np.random.shuffle(paths_data)\n",
    "            self.paths_data = paths_data[:5*self.n_pentagons]\n",
    "        self.array_data = np.array([np.load(str(path)) for path in self.paths_data])\n",
    "        self.array_data = self.array_data - np.array(const.IMAGENET_NORM_MEAN).reshape((1,3,1,1))\n",
    "        self.array_data = self.array_data / np.array(const.IMAGENET_NORM_STD).reshape((1,3,1,1))\n",
    "        \n",
    "    def iter_single_sample(self): \n",
    "        if self.mode == 'random':\n",
    "            iter_walk = sch.walk.walk_random(self.G, steps=self.max_steps)\n",
    "        elif self.mode == 'euclidean':\n",
    "            iter_walk = sch.walk.walk_euclidean(self.G)\n",
    "        elif self.mode == 'hamiltonian':\n",
    "            iter_walk = sch.walk.walk_hamiltonian(self.G)\n",
    "\n",
    "        for sample in iter_walk:\n",
    "            yield self.array_data[sample[0]], sample[0]\n",
    "        \n",
    "    def iter_batch_sample(self):\n",
    "        iter_batch = zip(*[self.iter_single_sample()\n",
    "                           for _ in range(self.batch_size)])\n",
    "        for batch in iter_batch:\n",
    "            data, nodes = zip(*batch)\n",
    "            yield data, nodes\n",
    "        \n",
    "    def iter_batch_dataset(self):   \n",
    "        for _ in range(self.n_paths):\n",
    "            data, nodes = zip(*list(self.iter_batch_sample()))\n",
    "            yield np.moveaxis(np.array(data), 0, 1), nodes\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self.iter_batch_dataset()\n",
    "    \n",
    "    def __getitem__(self, null):\n",
    "        data_iter = self.iter_batch_dataset()\n",
    "        data = next(data_iter)[0]\n",
    "        data2 = np.zeros(data.shape)\n",
    "        data2[:,:-1,:,:,:] = data[:,1:,:,:,:]\n",
    "        return data, data2\n",
    "    \n",
    "max_steps = 100\n",
    "epochs = 1\n",
    "batch_size = 14\n",
    "n_paths = 1\n",
    "\n",
    "iter_ds = ShapiroFractalsDataset(batch_size=batch_size, max_steps=max_steps, n_paths=n_paths,\n",
    "                                 mapping=mapping)\n",
    "for _ in range(epochs):\n",
    "    for i, batch in enumerate(iter_ds):\n",
    "        print(i, batch[0].shape)\n",
    "        if i == max_steps:\n",
    "            print('bad')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 3, 128, 160)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter_ds.array_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinitializing the Readout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = fine_tune_data.shape[1]\n",
    "orig_weights_file = str(ks.WEIGHTS_DIR / 'tensorflow_weights/prednet_kitti_weights.hdf5')  # original t+1 weights\n",
    "orig_json_file = str(ks.WEIGHTS_DIR / 'prednet_kitti_model.json')\n",
    "\n",
    "fractals_weights_file = str(ks.WEIGHTS_DIR / 'tensorflow_weights/prednet_kitti_weights-fractals_finetuned.hdf5')  # where new weights will be saved\n",
    "fractals_json_file = str(ks.WEIGHTS_DIR / 'prednet_kitti_model-fractals_finetuned.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = True\n",
    "nb_epoch = 200\n",
    "batch_size = 4\n",
    "samples_per_epoch = None\n",
    "N_seq_val = 50  # number of sequences to use for validation\n",
    "max_steps = 100\n",
    "n_paths = 100\n",
    "nt = max_steps\n",
    "\n",
    "# Load t+1 model\n",
    "f = open(orig_json_file, 'r')\n",
    "json_string = f.read()\n",
    "f.close()\n",
    "orig_model = model_from_json(json_string, custom_objects = {'PredNet': pn.PredNet})\n",
    "orig_model.load_weights(orig_weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6,\n",
       "  <tf.Variable 'prednet_1_3/layer_ahat_0/kernel:0' shape=(3, 3, 3, 3) dtype=float32>),\n",
       " (7, <tf.Variable 'prednet_1_3/layer_ahat_0/bias:0' shape=(3,) dtype=float32>)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readout_weights = [(i, w) for i, w in \n",
    "                   enumerate(orig_model.layers[1].trainable_weights) \n",
    "                   if 'ahat_0' in w.name]\n",
    "readout_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_weights = orig_model.layers[1].get_weights()\n",
    "assert all([np.all(w1 == w2) for w1, w2 in\n",
    "            zip(orig_model.layers[1].get_weights(), orig_weights)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in readout_weights:\n",
    "    new_w = np.random.rand(*w.shape.as_list())*np.sqrt(1/(sum(w.shape.as_list())))\n",
    "    assert new_w.shape == w.shape\n",
    "    orig_weights[i] = new_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not all([np.all(w1 == w2) for w1, w2 in \n",
    "                zip(orig_model.layers[1].get_weights(), orig_weights)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_config = orig_model.layers[1].get_config()\n",
    "layer_config['output_mode'] = 'prediction'\n",
    "data_format = layer_config['data_format'] if 'data_format' in layer_config else layer_config['dim_ordering']\n",
    "prednet = pn.PredNet(weights=orig_weights, **layer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://erikbrorson.github.io/2018/04/30/Adam-with-learning-rate-multipliers/\n",
    "from keras.legacy import interfaces\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Optimizer\n",
    "\n",
    "class Adam_lr_mult(Optimizer):\n",
    "    \"\"\"Adam optimizer.\n",
    "    Adam optimizer, with learning rate multipliers built on Keras implementation\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
    "            algorithm from the paper \"On the Convergence of Adam and\n",
    "            Beyond\".\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n",
    "        - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n",
    "        \n",
    "    AUTHOR: Erik Brorson\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., amsgrad=False,\n",
    "                 multipliers=None, debug_verbose=False,**kwargs):\n",
    "        super(Adam_lr_mult, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.learning_rate = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsgrad = amsgrad\n",
    "        self.multipliers = multipliers\n",
    "        self.debug_verbose = debug_verbose\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.learning_rate\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                  K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "\n",
    "            # Learning rate multipliers\n",
    "            if self.multipliers:\n",
    "                multiplier = [mult for mult in self.multipliers if mult in p.name]\n",
    "            else:\n",
    "                multiplier = None\n",
    "            if multiplier:\n",
    "                new_lr_t = lr_t * self.multipliers[multiplier[0]]\n",
    "                if self.debug_verbose:\n",
    "                    print('Setting {} to learning rate {}'.format(multiplier[0], new_lr_t))\n",
    "                    print(K.get_value(new_lr_t))\n",
    "            else:\n",
    "                new_lr_t = lr_t\n",
    "                if self.debug_verbose:\n",
    "                    print('No change in learning rate {}'.format(p.name))\n",
    "                    print(K.get_value(new_lr_t))\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                p_t = p - new_lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                p_t = p - new_lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.learning_rate)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'amsgrad': self.amsgrad,\n",
    "                  'multipliers':self.multipliers}\n",
    "        base_config = super(Adam_lr_mult, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_multipliers =  {w.name : 0.001 for w in orig_model.layers[1].trainable_weights}\n",
    "for i, w in readout_weights:\n",
    "    learning_rate_multipliers[w.name] = 1\n",
    "\n",
    "adam_lr_mult = Adam_lr_mult(multipliers=learning_rate_multipliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = list(orig_model.layers[0].batch_input_shape[1:])\n",
    "input_shape[0] = nt\n",
    "\n",
    "inputs = Input(input_shape)\n",
    "predictions = prednet(inputs)\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(loss='mse', optimizer=adam_lr_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mapping as follows:\n",
      "{0: '42', 1: '81', 2: '64', 3: '7', 4: '61', 5: '35', 6: '18', 7: '83', 8: '60', 9: '3', 10: '53', 11: '10', 12: '22', 13: '8', 14: '91'}\n",
      "WARNING:tensorflow:From /home/apra/miniconda3/envs/prednet/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/apra/miniconda3/envs/prednet/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/200\n"
     ]
    }
   ],
   "source": [
    "train_generator = ShapiroFractalsDataset(batch_size=batch_size, max_steps=max_steps, n_paths=n_paths,\n",
    "                                    mapping=mapping)\n",
    "\n",
    "lr_schedule = lambda epoch: 0.001 if epoch < 100 else 0.0001    # start with lr of 0.001 and then drop to 0.0001 after 75 epochs\n",
    "callbacks = [LearningRateScheduler(lr_schedule)]\n",
    "if save_model:\n",
    "    if not ks.WEIGHTS_DIR.exists(): \n",
    "        ks.WEIGHTS_DIR.mkdir(parents=True)\n",
    "    callbacks.append(ModelCheckpoint(filepath=fractals_weights_file, monitor='loss', save_best_only=True))\n",
    "history = model.fit_generator(train_generator, samples_per_epoch, nb_epoch, callbacks=callbacks,\n",
    "                validation_data=None)\n",
    "\n",
    "if save_model:\n",
    "    json_string = model.to_json()\n",
    "    with open(fractals_json_file, \"w\") as f:\n",
    "        f.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "gcf = plt.gcf()\n",
    "gcf.set_size_inches(16,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = 36\n",
    "# Create testing model (to output predictions)\n",
    "layer_config = model.layers[1].get_config()\n",
    "X_Rs = []\n",
    "\n",
    "for i in range(len(layer_config['R_stack_sizes'])):\n",
    "\n",
    "    layer_config['output_mode'] = 'R' + str(i)\n",
    "    data_format = layer_config['data_format'] if 'data_format' in layer_config \\\n",
    "        else layer_config['dim_ordering']\n",
    "    test_prednet = pn.PredNet(weights=model.layers[1].get_weights(), \n",
    "                              **layer_config)\n",
    "\n",
    "    input_shape = list(model.layers[0].batch_input_shape[1:])\n",
    "    input_shape[0] = nt\n",
    "    inputs = Input(shape=tuple(input_shape))\n",
    "    R_outs = test_prednet(inputs)\n",
    "    test_model = Model(inputs=inputs, outputs=R_outs)\n",
    "\n",
    "    X_Rs.append(test_model.predict(test_data, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Rs[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, node in enumerate(test_walk_nodes):\n",
    "    print(i, node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borders = [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G, with_labels=True, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs_Rs = []\n",
    "for i in range(len(X_Rs)):\n",
    "    diffs_Rs.append(np.mean(np.diff(X_Rs[i][0], axis=0)**2, axis=(1,2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax_large = fig.add_subplot(111)\n",
    "\n",
    "for i, diff_R in enumerate(diffs_Rs):\n",
    "    ax = fig.add_subplot(11 + i + len(diffs_Rs)*100)\n",
    "    ax.plot(diff_R)\n",
    "    ax.set_ylabel(f'Layer {i+1}')\n",
    "    [ax.axes.axvline(b, ls=':') for b in borders]\n",
    "    if i != len(diffs_Rs)-1:\n",
    "        ax.axes.xaxis.set_ticks([])\n",
    "        \n",
    "ax_large.axes.xaxis.set_ticks([])\n",
    "ax_large.axes.yaxis.set_ticks([])\n",
    "ax_large.set_xlabel('Step')\n",
    "gcf = plt.gcf()\n",
    "gcf.set_size_inches(16,9)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
