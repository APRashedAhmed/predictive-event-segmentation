{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0.3 Running PredNet on Fractal Data\n",
    "\n",
    "Getting the fractal data to work with the implemented prednet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [watermark](https://github.com/rasbt/watermark) to see the state of the machine and environment that's running the notebook. To make sense of the options, take a look at the [usage](https://github.com/rasbt/watermark#usage) section of the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 19 2020 23:57:59 \n",
      "\n",
      "compiler   : GCC 7.3.0\n",
      "system     : Linux\n",
      "release    : 5.4.0-42-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "host name  : apra-x3\n",
      "Git hash   : 6bb37327aadbf82b9ecf873db771478d92babd4b\n",
      "Git branch : master\n"
     ]
    }
   ],
   "source": [
    "# Load `watermark` extension\n",
    "%load_ext watermark\n",
    "# Display the status of the machine and other non-code related info\n",
    "%watermark -n -m -g -b -t -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [autoreload](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html) which will always reload modules marked with `%aimport`.\n",
    "\n",
    "This behavior can be inverted by running `autoreload 2` which will set everything to be auto-reloaded *except* for modules marked with `%aimport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `autoreload` extension\n",
    "%load_ext autoreload\n",
    "# Set autoreload behavior\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `matplotlib` in one of the more `jupyter`-friendly [rich-output modes](https://ipython.readthedocs.io/en/stable/interactive/plotting.html). Some options (that may or may not have worked) are `inline`, `notebook`, and `gtk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the matplotlib mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging           0.5.1.2\n",
      "prevseg           0+untagged.39.g6bb3732.dirty\n",
      "PIL.Image         7.2.0\n",
      "torch             1.6.0\n",
      "numpy             1.19.1\n",
      "networkx          2.4\n",
      "pytorch_lightning 0.8.5\n",
      "CPython 3.8.5\n",
      "IPython 7.16.1\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import logging\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image, ImageOps\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "%aimport prevseg.constants\n",
    "import prevseg.constants as const\n",
    "%aimport prevseg.index\n",
    "import prevseg.index as index\n",
    "%aimport prevseg.dataloaders.schapiro\n",
    "import prevseg.dataloaders.schapiro as sch\n",
    "%aimport prevseg.schapiro\n",
    "from prevseg.schapiro import walk, graph\n",
    "%aimport prevseg.models.prednet\n",
    "import prevseg.models.prednet as prednet\n",
    "\n",
    "# Keep track of versions of everything\n",
    "%watermark -v -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Fixed Path-Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mapping as follows:\n",
      "{0: '47', 1: '9', 2: '68', 3: '16', 4: '71', 5: '98', 6: '1', 7: '48', 8: '42', 9: '61', 10: '88', 11: '97', 12: '96', 13: '65', 14: '69'}\n",
      "0 torch.Size([2, 128, 128])\n",
      "1 torch.Size([2, 128, 128])\n",
      "0 torch.Size([2, 128, 128])\n",
      "1 torch.Size([2, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "max_steps = 2\n",
    "epochs = 2\n",
    "batch_size = 2\n",
    "\n",
    "iter_ds = sch.ShapiroFractalsDataset(batch_size=batch_size, max_steps=max_steps)\n",
    "loader = DataLoader(iter_ds, batch_size=None)\n",
    "for _ in range(epochs):\n",
    "    for i, batch in enumerate(loader):\n",
    "        print(i, batch.shape)\n",
    "        if i > max_steps+5:\n",
    "            print('bad')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mapping as follows:\n",
      "{0: '17', 1: '59', 2: '97', 3: '20', 4: '50', 5: '78', 6: '28', 7: '5', 8: '61', 9: '16', 10: '57', 11: '63', 12: '66', 13: '80', 14: '99'}\n",
      "0 torch.Size([2, 10, 128, 128])\n",
      "1 torch.Size([2, 10, 128, 128])\n",
      "2 torch.Size([2, 10, 128, 128])\n",
      "3 torch.Size([2, 10, 128, 128])\n",
      "4 torch.Size([2, 10, 128, 128])\n",
      "0 torch.Size([2, 10, 128, 128])\n",
      "1 torch.Size([2, 10, 128, 128])\n",
      "2 torch.Size([2, 10, 128, 128])\n",
      "3 torch.Size([2, 10, 128, 128])\n",
      "4 torch.Size([2, 10, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "class ShapiroFractalsDataset(IterableDataset):\n",
    "    def __init__(self, batch_size=32, n_pentagons=3, max_steps=128, n_paths=128, mapping=None):\n",
    "        self.batch_size = batch_size\n",
    "        self.n_pentagons = n_pentagons\n",
    "        self.max_steps = max_steps\n",
    "        self.n_paths = n_paths\n",
    "        self.mapping = mapping\n",
    "        \n",
    "        self.G = graph.schapiro_graph(n_pentagons=n_pentagons)\n",
    "        \n",
    "        self.load_node_stimuli()\n",
    "        \n",
    "        self.mapping = {node : path.stem\n",
    "                        for node, path in zip(range(len(self.G.nodes)),\n",
    "                                              self.paths_data)}\n",
    "        print(f'Created mapping as follows:\\n{self.mapping}')\n",
    "        \n",
    "    def load_node_stimuli(self):\n",
    "        # Load the fractal images into memory\n",
    "        assert index.DIR_SCH_FRACTALS.exists()\n",
    "        if self.mapping:\n",
    "            self.paths_data = [index.DIR_SCH_FRACTALS / (name+'.tiff')\n",
    "                               for name in self.mapping.values()]\n",
    "        else:\n",
    "            paths_data = list(index.DIR_SCH_FRACTALS.iterdir())\n",
    "            np.random.shuffle(paths_data)\n",
    "            self.paths_data = paths_data[:5*self.n_pentagons]\n",
    "        self.array_data = np.array(\n",
    "            [np.array(ImageOps.grayscale(Image.open(str(path))))\n",
    "             for path in self.paths_data])\n",
    "        \n",
    "    def iter_batch_sample(self):\n",
    "        def iter_single_sample():\n",
    "            for sample in walk.walk_random(self.G, steps=self.max_steps):\n",
    "                yield self.array_data[sample[0]]\n",
    "                #yield sample[0]\n",
    "        yield from zip(*[iter_single_sample() for _ in range(self.batch_size)])\n",
    "        \n",
    "    def iter_batch_dataset(self):       \n",
    "        for _ in range(self.n_paths):\n",
    "            yield np.moveaxis(np.array(list(self.iter_batch_sample())), 0, 1)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self.iter_batch_dataset()\n",
    "\n",
    "    \n",
    "max_steps = 10\n",
    "epochs = 2\n",
    "batch_size = 2\n",
    "n_paths = 5\n",
    "\n",
    "iter_ds = ShapiroFractalsDataset(batch_size=batch_size, max_steps=max_steps, n_paths=n_paths)\n",
    "loader = DataLoader(iter_ds, batch_size=None)\n",
    "for _ in range(epochs):\n",
    "    for i, batch in enumerate(loader):\n",
    "        print(i, batch.shape)\n",
    "        if i > max_steps+5:\n",
    "            print('bad')\n",
    "            break\n",
    "            \n",
    "mapping = iter_ds.mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mapping as follows:\n",
      "{0: '17', 1: '59', 2: '97', 3: '20', 4: '50', 5: '78', 6: '28', 7: '5', 8: '61', 9: '16', 10: '57', 11: '63', 12: '66', 13: '80', 14: '99'}\n",
      "0 torch.Size([2, 10, 2048])\n",
      "1 torch.Size([2, 10, 2048])\n",
      "2 torch.Size([2, 10, 2048])\n",
      "3 torch.Size([2, 10, 2048])\n",
      "4 torch.Size([2, 10, 2048])\n",
      "0 torch.Size([2, 10, 2048])\n",
      "1 torch.Size([2, 10, 2048])\n",
      "2 torch.Size([2, 10, 2048])\n",
      "3 torch.Size([2, 10, 2048])\n",
      "4 torch.Size([2, 10, 2048])\n"
     ]
    }
   ],
   "source": [
    "max_steps = 10\n",
    "epochs = 2\n",
    "batch_size = 2\n",
    "n_paths = 5\n",
    "\n",
    "class ShapiroResnetEmbeddingDataset(ShapiroFractalsDataset):\n",
    "    def load_node_stimuli(self):\n",
    "        # Load the fractal images into memory\n",
    "        assert index.DIR_SCH_FRACTALS.exists()\n",
    "        if self.mapping:\n",
    "            self.paths_data = [index.DIR_SCH_FRACTALS_EMB / (name+'.npy')\n",
    "                               for name in self.mapping.values()]\n",
    "        else:\n",
    "            paths_data = list(index.DIR_SCH_FRACTALS_EMB.iterdir())\n",
    "            np.random.shuffle(paths_data)\n",
    "            self.paths_data = paths_data[:5*self.n_pentagons]\n",
    "        self.array_data = np.array(\n",
    "            [np.array(np.load(str(path)))\n",
    "             for path in self.paths_data])    \n",
    "\n",
    "iter_ds = ShapiroResnetEmbeddingDataset(\n",
    "    batch_size=batch_size, \n",
    "    max_steps=max_steps, \n",
    "    n_paths=n_paths,\n",
    "    mapping=mapping,\n",
    ")\n",
    "loader = DataLoader(iter_ds, batch_size=None)\n",
    "for _ in range(epochs):\n",
    "    for i, batch in enumerate(loader):\n",
    "        print(i, batch.shape)\n",
    "        if i > max_steps+5:\n",
    "            print('bad')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PredNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredNetTracked(prednet.PredNetTracked):\n",
    "    @prednet.PredNet.timeit\n",
    "    def prepare_data(self):\n",
    "        self.ds = self.ds or ShapiroResnetEmbeddingDataset(\n",
    "            batch_size=self.batch_size, \n",
    "            n_pentagons=self.hparams.n_pentagons, \n",
    "            max_steps=self.hparams.max_steps, \n",
    "            n_paths=self.hparams.n_paths)\n",
    "        self.ds_val = ShapiroResnetEmbeddingDataset(\n",
    "            batch_size=self.batch_size, \n",
    "            n_pentagons=self.hparams.n_pentagons, \n",
    "            max_steps=self.hparams.max_steps, \n",
    "            n_paths=1,\n",
    "            mapping=self.ds.mapping)\n",
    "        \n",
    "#         n_test, n_val = self.hparams.n_test, self.hparams.n_val\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.ds_val, \n",
    "                          batch_size=None,\n",
    "                          num_workers=self.hparams.n_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.ds, \n",
    "                          batch_size=None,\n",
    "                          num_workers=self.hparams.n_workers)\n",
    "    \n",
    "    def _common_step(self, batch, batch_idx, mode):\n",
    "        data = batch\n",
    "        errors = self.forward(data) # batch x n_layers x nt\n",
    "\n",
    "        loc_batch = errors.size(0)\n",
    "        errors = torch.mm(errors.view(-1, self.time_steps), \n",
    "                          self.time_loss_weights) # batch*n_layers x 1\n",
    "        errors = torch.mm(errors.view(loc_batch, -1), \n",
    "                          self.layer_loss_weights)\n",
    "        errors = torch.mean(errors, axis=0)\n",
    "        \n",
    "        if mode == 'train':\n",
    "            prefix = ''\n",
    "        else:\n",
    "            prefix = mode + '_'\n",
    "            \n",
    "        self.logger.experiment.add_scalar(f'{prefix}loss', \n",
    "                                          errors, self.global_step)\n",
    "        return {f'{prefix}loss' : errors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: : 8it [10:09, 76.19s/it, loss=0.185, v_num=11]\n",
      "Epoch 1: : 12it [08:03, 40.29s/it, loss=0.212, v_num=12]\n",
      "\n",
      "                                  \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "model, trainer = None, None\n",
    "train_dataloader, val_dataloader = None, None\n",
    "errors, optimizer = None, None\n",
    "ckpt = None\n",
    "train_errors, val_errors = None, None\n",
    "res = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "ModelClass = PredNetTracked\n",
    "hparams = const.DEFAULT_HPARAMS\n",
    "hparams.n_layers = 2\n",
    "hparams.batch_size = 256\n",
    "hparams.max_steps = 2\n",
    "hparams.n_paths = 2\n",
    "hparams.n_pentagons = 3\n",
    "hparams.time_steps = hparams.max_steps\n",
    "hparams.exp_name = 'schapiro_test'\n",
    "hparams.name = f'{ModelClass.name}_{hparams.exp_name}'\n",
    "\n",
    "log_dir = Path(hparams.dir_logs) / f'{hparams.name}'\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir(parents=True)\n",
    "logger = pl.loggers.TensorBoardLogger(str(log_dir.parent), name=hparams.name)\n",
    "\n",
    "ckpt_dir = Path(hparams.dir_checkpoints) / f'{hparams.name}_v{logger.version}'\n",
    "if not ckpt_dir.exists():\n",
    "    ckpt_dir.mkdir(parents=True)\n",
    "    \n",
    "ckpt = pl.callbacks.ModelCheckpoint(\n",
    "    filepath=str(ckpt_dir / (hparams.exp_name+'_{global_step:05d}_{epoch:03d}_{val_loss:.3f}')),\n",
    "    monitor='loss',\n",
    "    verbose=True,\n",
    "    save_top_k=1,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(checkpoint_callback=ckpt,\n",
    "                     max_epochs=2,\n",
    "                     logger=logger,\n",
    "                     gpus=1\n",
    "                     )\n",
    "\n",
    "model = ModelClass(hparams)\n",
    "model.ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                 | Type       | Params\n",
      "----------------------------------------------------\n",
      "0 | predcell_0_recurrent | LSTM       | 67 M  \n",
      "1 | predcell_0_dense     | Sequential | 4 M   \n",
      "2 | predcell_0_update_a  | Sequential | 4 M   \n",
      "3 | predcell_1_recurrent | LSTM       | 12 M  \n",
      "4 | predcell_1_dense     | Sequential | 1 M   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mapping as follows:\n",
      "{0: '1', 1: '60', 2: '95', 3: '100', 4: '14', 5: '2', 6: '63', 7: '58', 8: '96', 9: '55', 10: '99', 11: '50', 12: '7', 13: '89', 14: '12'}\n",
      "Created mapping as follows:\n",
      "{0: '1', 1: '60', 2: '95', 3: '100', 4: '14', 5: '2', 6: '63', 7: '58', 8: '96', 9: '55', 10: '99', 11: '50', 12: '7', 13: '89', 14: '12'}\n",
      "Epoch 1: : 4it [00:00,  8.33it/s, loss=0.203, v_num=13]               \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: : 5it [00:00,  8.33it/s, loss=0.203, v_num=13]\n",
      "Epoch 1: : 10it [00:00, 13.84it/s, loss=0.203, v_num=13]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to Tensor.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-45d4afebefac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/g2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tpu\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no-cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/g2/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_parts.py\u001b[0m in \u001b[0;36msingle_gpu_train\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreinit_scheduler_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_schedulers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/g2/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m         \u001b[0;31m# CORE TRAINING LOOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m     def test(\n",
      "\u001b[0;32m~/miniconda3/envs/g2/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0;31m# RUN TNG EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/g2/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mshould_check_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_check_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_last_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_dev_run\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mshould_check_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# -----------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/g2/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(self, test_mode)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_validation_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/g2/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py\u001b[0m in \u001b[0;36mon_validation_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Called when the validation loop ends.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_validation_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/g2/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/g2/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36mon_validation_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    307\u001b[0m                 )\n\u001b[1;32m    308\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_monitor_top_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_check_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nEpoch {epoch:05d}: {self.monitor}  was not in top {self.save_top_k}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/g2/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_do_check_save\u001b[0;34m(self, filepath, current, epoch)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             log.info(\n\u001b[0;32m--> 343\u001b[0;31m                 \u001b[0;34mf'\\nEpoch {epoch:05d}: {self.monitor} reached'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m                 \u001b[0;34mf' {current:0.5f} (best {self.best_model_score:0.5f}), saving model to'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 f' {filepath} as top {self.save_top_k}')\n",
      "\u001b[0;32m~/miniconda3/envs/g2/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ipow__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to Tensor.__format__"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: : 12it [00:20,  1.67s/it, loss=0.203, v_num=13]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
