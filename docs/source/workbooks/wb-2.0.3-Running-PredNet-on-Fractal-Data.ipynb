{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0.3 Running PredNet on Fractal Data\n",
    "\n",
    "Getting the fractal data to work with the implemented prednet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [watermark](https://github.com/rasbt/watermark) to see the state of the machine and environment that's running the notebook. To make sense of the options, take a look at the [usage](https://github.com/rasbt/watermark#usage) section of the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 20 2020 10:58:46 \n",
      "\n",
      "compiler   : GCC 7.3.0\n",
      "system     : Linux\n",
      "release    : 5.4.0-42-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "host name  : apra-x3\n",
      "Git hash   : 45cc25f141a3ec65725320f89cf953e8381ed860\n",
      "Git branch : master\n"
     ]
    }
   ],
   "source": [
    "# Load `watermark` extension\n",
    "%load_ext watermark\n",
    "# Display the status of the machine and other non-code related info\n",
    "%watermark -n -m -g -b -t -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [autoreload](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html) which will always reload modules marked with `%aimport`.\n",
    "\n",
    "This behavior can be inverted by running `autoreload 2` which will set everything to be auto-reloaded *except* for modules marked with `%aimport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `autoreload` extension\n",
    "%load_ext autoreload\n",
    "# Set autoreload behavior\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `matplotlib` in one of the more `jupyter`-friendly [rich-output modes](https://ipython.readthedocs.io/en/stable/interactive/plotting.html). Some options (that may or may not have worked) are `inline`, `notebook`, and `gtk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the matplotlib mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prevseg           0+untagged.40.g45cc25f.dirty\n",
      "pytorch_lightning 0.8.5\n",
      "PIL.Image         7.2.0\n",
      "torch             1.6.0\n",
      "networkx          2.4\n",
      "logging           0.5.1.2\n",
      "numpy             1.19.1\n",
      "CPython 3.8.5\n",
      "IPython 7.16.1\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import logging\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from PIL import Image, ImageOps\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "%aimport prevseg.constants\n",
    "import prevseg.constants as const\n",
    "%aimport prevseg.index\n",
    "import prevseg.index as index\n",
    "%aimport prevseg.dataloaders.schapiro\n",
    "import prevseg.dataloaders.schapiro as sch\n",
    "%aimport prevseg.schapiro\n",
    "from prevseg.schapiro import walk, graph\n",
    "%aimport prevseg.models.prednet\n",
    "import prevseg.models.prednet as prednet\n",
    "\n",
    "# Keep track of versions of everything\n",
    "%watermark -v -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Fixed Path-Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mapping as follows:\n",
      "{0: '94', 1: '100', 2: '71', 3: '37', 4: '28', 5: '67', 6: '57', 7: '27', 8: '2', 9: '36', 10: '78', 11: '42', 12: '41', 13: '13', 14: '77'}\n",
      "0 torch.Size([2, 128, 128])\n",
      "1 torch.Size([2, 128, 128])\n",
      "0 torch.Size([2, 128, 128])\n",
      "1 torch.Size([2, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "max_steps = 2\n",
    "epochs = 2\n",
    "batch_size = 2\n",
    "\n",
    "iter_ds = sch.ShapiroFractalsDataset(batch_size=batch_size, max_steps=max_steps)\n",
    "loader = DataLoader(iter_ds, batch_size=None)\n",
    "for _ in range(epochs):\n",
    "    for i, batch in enumerate(loader):\n",
    "        print(i, batch.shape)\n",
    "        if i > max_steps+5:\n",
    "            print('bad')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mapping as follows:\n",
      "{0: '90', 1: '94', 2: '6', 3: '35', 4: '95', 5: '25', 6: '45', 7: '62', 8: '37', 9: '19', 10: '13', 11: '85', 12: '58', 13: '18', 14: '29'}\n",
      "0 torch.Size([2, 10, 128, 128])\n",
      "1 torch.Size([2, 10, 128, 128])\n",
      "2 torch.Size([2, 10, 128, 128])\n",
      "3 torch.Size([2, 10, 128, 128])\n",
      "4 torch.Size([2, 10, 128, 128])\n",
      "0 torch.Size([2, 10, 128, 128])\n",
      "1 torch.Size([2, 10, 128, 128])\n",
      "2 torch.Size([2, 10, 128, 128])\n",
      "3 torch.Size([2, 10, 128, 128])\n",
      "4 torch.Size([2, 10, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "class ShapiroFractalsDataset(IterableDataset):\n",
    "    def __init__(self, batch_size=32, n_pentagons=3, max_steps=128, n_paths=128, mapping=None):\n",
    "        self.batch_size = batch_size\n",
    "        self.n_pentagons = n_pentagons\n",
    "        self.max_steps = max_steps\n",
    "        self.n_paths = n_paths\n",
    "        self.mapping = mapping\n",
    "        \n",
    "        self.G = graph.schapiro_graph(n_pentagons=n_pentagons)\n",
    "        \n",
    "        self.load_node_stimuli()\n",
    "        \n",
    "        self.mapping = {node : path.stem\n",
    "                        for node, path in zip(range(len(self.G.nodes)),\n",
    "                                              self.paths_data)}\n",
    "        print(f'Created mapping as follows:\\n{self.mapping}')\n",
    "        \n",
    "    def load_node_stimuli(self):\n",
    "        # Load the fractal images into memory\n",
    "        assert index.DIR_SCH_FRACTALS.exists()\n",
    "        if self.mapping:\n",
    "            self.paths_data = [index.DIR_SCH_FRACTALS / (name+'.tiff')\n",
    "                               for name in self.mapping.values()]\n",
    "        else:\n",
    "            paths_data = list(index.DIR_SCH_FRACTALS.iterdir())\n",
    "            np.random.shuffle(paths_data)\n",
    "            self.paths_data = paths_data[:5*self.n_pentagons]\n",
    "        self.array_data = np.array(\n",
    "            [np.array(ImageOps.grayscale(Image.open(str(path))))\n",
    "             for path in self.paths_data])\n",
    "        \n",
    "    def iter_batch_sample(self):\n",
    "        def iter_single_sample():\n",
    "            for sample in walk.walk_random(self.G, steps=self.max_steps):\n",
    "                yield self.array_data[sample[0]]\n",
    "                #yield sample[0]\n",
    "        yield from zip(*[iter_single_sample() for _ in range(self.batch_size)])\n",
    "        \n",
    "    def iter_batch_dataset(self):       \n",
    "        for _ in range(self.n_paths):\n",
    "            yield np.moveaxis(np.array(list(self.iter_batch_sample())), 0, 1)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self.iter_batch_dataset()\n",
    "\n",
    "    \n",
    "max_steps = 10\n",
    "epochs = 2\n",
    "batch_size = 2\n",
    "n_paths = 5\n",
    "\n",
    "iter_ds = ShapiroFractalsDataset(batch_size=batch_size, max_steps=max_steps, n_paths=n_paths)\n",
    "loader = DataLoader(iter_ds, batch_size=None)\n",
    "for _ in range(epochs):\n",
    "    for i, batch in enumerate(loader):\n",
    "        print(i, batch.shape)\n",
    "        if i > max_steps+5:\n",
    "            print('bad')\n",
    "            break\n",
    "            \n",
    "mapping = iter_ds.mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mapping as follows:\n",
      "{0: '90', 1: '94', 2: '6', 3: '35', 4: '95', 5: '25', 6: '45', 7: '62', 8: '37', 9: '19', 10: '13', 11: '85', 12: '58', 13: '18', 14: '29'}\n",
      "0 torch.Size([2, 10, 2048])\n",
      "1 torch.Size([2, 10, 2048])\n",
      "2 torch.Size([2, 10, 2048])\n",
      "3 torch.Size([2, 10, 2048])\n",
      "4 torch.Size([2, 10, 2048])\n",
      "0 torch.Size([2, 10, 2048])\n",
      "1 torch.Size([2, 10, 2048])\n",
      "2 torch.Size([2, 10, 2048])\n",
      "3 torch.Size([2, 10, 2048])\n",
      "4 torch.Size([2, 10, 2048])\n"
     ]
    }
   ],
   "source": [
    "max_steps = 10\n",
    "epochs = 2\n",
    "batch_size = 2\n",
    "n_paths = 5\n",
    "\n",
    "class ShapiroResnetEmbeddingDataset(ShapiroFractalsDataset):\n",
    "    def load_node_stimuli(self):\n",
    "        # Load the fractal images into memory\n",
    "        assert index.DIR_SCH_FRACTALS.exists()\n",
    "        if self.mapping:\n",
    "            self.paths_data = [index.DIR_SCH_FRACTALS_EMB / (name+'.npy')\n",
    "                               for name in self.mapping.values()]\n",
    "        else:\n",
    "            paths_data = list(index.DIR_SCH_FRACTALS_EMB.iterdir())\n",
    "            np.random.shuffle(paths_data)\n",
    "            self.paths_data = paths_data[:5*self.n_pentagons]\n",
    "        self.array_data = np.array(\n",
    "            [np.array(np.load(str(path)))\n",
    "             for path in self.paths_data])    \n",
    "\n",
    "iter_ds = ShapiroResnetEmbeddingDataset(\n",
    "    batch_size=batch_size, \n",
    "    max_steps=max_steps, \n",
    "    n_paths=n_paths,\n",
    "    mapping=mapping,\n",
    ")\n",
    "loader = DataLoader(iter_ds, batch_size=None)\n",
    "for _ in range(epochs):\n",
    "    for i, batch in enumerate(loader):\n",
    "        print(i, batch.shape)\n",
    "        if i > max_steps+5:\n",
    "            print('bad')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PredNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredNetTracked(prednet.PredNetTracked):\n",
    "    @prednet.PredNet.timeit\n",
    "    def prepare_data(self):\n",
    "        self.ds = self.ds or ShapiroResnetEmbeddingDataset(\n",
    "            batch_size=self.batch_size, \n",
    "            n_pentagons=self.hparams.n_pentagons, \n",
    "            max_steps=self.hparams.max_steps, \n",
    "            n_paths=self.hparams.n_paths)\n",
    "        self.ds_val = ShapiroResnetEmbeddingDataset(\n",
    "            batch_size=self.batch_size, \n",
    "            n_pentagons=self.hparams.n_pentagons, \n",
    "            max_steps=self.hparams.max_steps, \n",
    "            n_paths=1,\n",
    "            mapping=self.ds.mapping)\n",
    "        \n",
    "#         n_test, n_val = self.hparams.n_test, self.hparams.n_val\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.ds_val, \n",
    "                          batch_size=None,\n",
    "                          num_workers=self.hparams.n_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.ds, \n",
    "                          batch_size=None,\n",
    "                          num_workers=self.hparams.n_workers)\n",
    "    \n",
    "    def _common_step(self, batch, batch_idx, mode):\n",
    "        data = batch\n",
    "        errors = self.forward(data) # batch x n_layers x nt\n",
    "\n",
    "        loc_batch = errors.size(0)\n",
    "        errors = torch.mm(errors.view(-1, self.time_steps), \n",
    "                          self.time_loss_weights) # batch*n_layers x 1\n",
    "        errors = torch.mm(errors.view(loc_batch, -1), \n",
    "                          self.layer_loss_weights)\n",
    "        errors = torch.mean(errors, axis=0)\n",
    "        \n",
    "        if mode == 'train':\n",
    "            prefix = ''\n",
    "        else:\n",
    "            prefix = mode + '_'\n",
    "            \n",
    "        self.logger.experiment.add_scalar(f'{prefix}loss', \n",
    "                                          errors, self.global_step)\n",
    "        return {f'{prefix}loss' : errors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "model, trainer = None, None\n",
    "train_dataloader, val_dataloader = None, None\n",
    "errors, optimizer = None, None\n",
    "ckpt = None\n",
    "train_errors, val_errors = None, None\n",
    "res = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "ModelClass = PredNetTracked\n",
    "hparams = const.DEFAULT_HPARAMS\n",
    "hparams.n_layers = 2\n",
    "hparams.batch_size = 256\n",
    "hparams.max_steps = 2\n",
    "hparams.n_paths = 2\n",
    "hparams.n_pentagons = 3\n",
    "hparams.time_steps = hparams.max_steps\n",
    "hparams.exp_name = 'schapiro_test'\n",
    "hparams.name = f'{ModelClass.name}_{hparams.exp_name}'\n",
    "\n",
    "log_dir = Path(hparams.dir_logs) / f'{hparams.name}'\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir(parents=True)\n",
    "logger = pl.loggers.TensorBoardLogger(str(log_dir.parent), name=hparams.name)\n",
    "\n",
    "ckpt_dir = Path(hparams.dir_checkpoints) / f'{hparams.name}_v{logger.version}'\n",
    "if not ckpt_dir.exists():\n",
    "    ckpt_dir.mkdir(parents=True)\n",
    "    \n",
    "ckpt = pl.callbacks.ModelCheckpoint(\n",
    "    filepath=str(ckpt_dir / (hparams.exp_name+'_{global_step:05d}_{epoch:03d}_{val_loss:.3f}')),\n",
    "    verbose=True,\n",
    "    save_top_k=1,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(checkpoint_callback=ckpt,\n",
    "                     max_epochs=2,\n",
    "                     logger=logger,\n",
    "                     gpus=1\n",
    "                     )\n",
    "\n",
    "model = ModelClass(hparams)\n",
    "model.ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name                 | Type       | Params\n",
      "----------------------------------------------------\n",
      "0 | predcell_0_recurrent | LSTM       | 67 M  \n",
      "1 | predcell_0_dense     | Sequential | 4 M   \n",
      "2 | predcell_0_update_a  | Sequential | 4 M   \n",
      "3 | predcell_1_recurrent | LSTM       | 12 M  \n",
      "4 | predcell_1_dense     | Sequential | 1 M   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mapping as follows:\n",
      "{0: '1', 1: '60', 2: '95', 3: '100', 4: '14', 5: '2', 6: '63', 7: '58', 8: '96', 9: '55', 10: '99', 11: '50', 12: '7', 13: '89', 14: '12'}\n",
      "Created mapping as follows:\n",
      "{0: '1', 1: '60', 2: '95', 3: '100', 4: '14', 5: '2', 6: '63', 7: '58', 8: '96', 9: '55', 10: '99', 11: '50', 12: '7', 13: '89', 14: '12'}\n",
      "Validation sanity check:  50%|█████     | 1/2 [00:00<00:00,  9.85it/s]<class 'torch.Tensor'>\n",
      "Epoch 1: : 4it [00:00,  8.74it/s, loss=0.203, v_num=24]               \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: : 5it [00:00,  8.68it/s, loss=0.203, v_num=24]\n",
      "Epoch 1: : 9it [00:00, 13.29it/s, loss=0.203, v_num=24]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: val_loss reached 0.18061 (best 0.18061), saving model to /home/apra/work/predictive-event-segmentation/models/checkpoints/prednet_tracked_schapiro_test_v24/schapiro_test_global_step=00003_epoch=000_val_loss=0.181.ckpt as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Epoch 1: : 12it [00:03,  3.46it/s, loss=0.203, v_num=24]\n",
      "Epoch 2: : 4it [00:00,  9.06it/s, loss=0.185, v_num=24] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: : 8it [00:00, 12.85it/s, loss=0.185, v_num=24]\n",
      "Validating: 5it [00:00, 12.81it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss reached 0.15046 (best 0.15046), saving model to /home/apra/work/predictive-event-segmentation/models/checkpoints/prednet_tracked_schapiro_test_v24/schapiro_test_global_step=00007_epoch=001_val_loss=0.150.ckpt as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Epoch 2: : 12it [00:03,  3.34it/s, loss=0.185, v_num=24]\n",
      "Epoch 2: : 12it [00:03,  3.33it/s, loss=0.185, v_num=24]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
