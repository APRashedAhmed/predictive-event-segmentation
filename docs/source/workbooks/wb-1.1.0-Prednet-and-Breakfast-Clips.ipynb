{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1.0 Prednet and Breakfast Clips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [watermark](https://github.com/rasbt/watermark) to see the state of the machine and environment that's running the notebook. To make sense of the options, take a look at the [usage](https://github.com/rasbt/watermark#usage) section of the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 26 2020 17:58:59 \n",
      "\n",
      "CPython 3.6.10\n",
      "IPython 7.12.0\n",
      "\n",
      "torch 1.2.0\n",
      "torchvision 0.1.8\n",
      "cv2 3.4.2\n",
      "h5py 2.8.0\n",
      "pandas 1.0.1\n",
      "matplotlib 3.1.3\n",
      "seaborn 0.10.0\n",
      "jupyterlab 1.2.6\n",
      "lab 0+untagged.29.g1bb7899.dirty\n",
      "\n",
      "compiler   : GCC 7.3.0\n",
      "system     : Linux\n",
      "release    : 4.15.0-76-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n",
      "Git hash   : 1bb789928298409106529d6e8942da764a7e8865\n",
      "Git branch : master\n"
     ]
    }
   ],
   "source": [
    "# Load `watermark` extension\n",
    "%load_ext watermark\n",
    "# Display the status of the machine and packages. Add more as necessary.\n",
    "%watermark -v -n -m -g -b -t -p torch,torchvision,cv2,h5py,pandas,matplotlib,seaborn,jupyterlab,lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [autoreload](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html) which will always reload modules marked with `%aimport`.\n",
    "\n",
    "This behavior can be inverted by running `autoreload 2` which will set everything to be auto-reloaded *except* for modules marked with `%aimport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `autoreload` extension\n",
    "%load_ext autoreload\n",
    "# Set autoreload behavior\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `matplotlib` in one of the more `jupyter`-friendly [rich-output modes](https://ipython.readthedocs.io/en/stable/interactive/plotting.html). Some options (that may or may not have worked) are `inline`, `notebook`, and `gtk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the matplotlib mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the GPU\n",
    "\n",
    "Make sure we aren't greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 26 17:59:14 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN Xp            Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 44%   70C    P2   190W / 250W |  11851MiB / 12196MiB |     56%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN Xp            Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 51%   80C    P2   119W / 250W |  11851MiB / 12196MiB |     53%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN Xp            Off  | 00000000:08:00.0 Off |                  N/A |\n",
      "| 54%   81C    P2    84W / 250W |  11851MiB / 12196MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  TITAN Xp            Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 54%   83C    P2   161W / 250W |  11851MiB / 12196MiB |     51%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  TITAN Xp            Off  | 00000000:84:00.0 Off |                  N/A |\n",
      "| 31%   53C    P2    63W / 250W |   1557MiB / 12196MiB |     23%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  TITAN Xp            Off  | 00000000:85:00.0 Off |                  N/A |\n",
      "| 47%   74C    P2   120W / 250W |  11851MiB / 12196MiB |     54%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  TITAN Xp            Off  | 00000000:88:00.0 Off |                  N/A |\n",
      "| 27%   48C    P2    62W / 250W |   2099MiB / 12196MiB |      8%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  TITAN Xp            Off  | 00000000:89:00.0 Off |                  N/A |\n",
      "| 50%   78C    P2   199W / 250W |  11851MiB / 12196MiB |     45%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     27664      C   python                                     11841MiB |\n",
      "|    1      1128      C   python                                     11841MiB |\n",
      "|    2     20088      C   python                                     11841MiB |\n",
      "|    3      5491      C   python                                     11841MiB |\n",
      "|    4     31296      C   python                                      1547MiB |\n",
      "|    5     14547      C   python                                     11841MiB |\n",
      "|    6     23375      C   python                                      2089MiB |\n",
      "|    7      5031      C   python                                     11841MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local imports that may or may not be autoreloaded. This section contains things that will likely have to be re-imported multiple times, and have additions or subtractions made throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants to be used throughout the package\n",
    "%aimport lab\n",
    "import lab\n",
    "%aimport lab.index\n",
    "from lab.index import DIR_DATA_INT, DIR_DATA_RAW\n",
    "%aimport lab.breakfast\n",
    "import lab.breakfast as bk\n",
    "%aimport lab.breakfast.constants\n",
    "from lab.breakfast.constants import SEED\n",
    "# Import the data subdirectories\n",
    "%aimport lab.breakfast.index\n",
    "from lab.breakfast.index import (DIR_BREAKFAST, \n",
    "                                 DIR_BREAKFAST_DATA, \n",
    "                                 DIR_COARSE_SEG, \n",
    "                                 DIR_FINE_SEG,\n",
    "                                )\n",
    "%aimport lab.breakfast.dataloader\n",
    "from lab.breakfast.dataloader import Breakfast64DimFVDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set [seaborn defaults](https://seaborn.pydata.org/generated/seaborn.set.html) for matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Breakfast64DimFVDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-4.930377, -3.729419, -2.974148, ...,  4.952208, -0.0524  ,\n",
       "         -2.923864],\n",
       "        [-4.745723, -3.4886  , -3.447279, ...,  5.638719, -1.229474,\n",
       "         -2.064759],\n",
       "        [-4.840712, -3.733112, -3.627248, ...,  5.671757, -1.623616,\n",
       "         -1.68041 ],\n",
       "        ...,\n",
       "        [-2.588912,  0.885864, -1.724053, ...,  2.534779, -7.805843,\n",
       "         21.473339],\n",
       "        [-1.485294,  0.796884, -2.246547, ...,  1.793978, -8.247074,\n",
       "         22.106295],\n",
       "        [-1.318826,  0.558616, -2.55644 , ...,  1.375821, -8.771461,\n",
       "         22.229328]]),\n",
       " '/media/data_cifs2/apra/work/labwork/data/interim/breakfast/video_clips/event_clips/64dim_fv_clips/clips/38_pancake_stereo_1_start_1244_event_55_seed_117.npy')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data\n",
    "\n",
    "See [this](https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets/50544887#50544887) stackoverflow post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "ds_length = len(ds)\n",
    "indices = list(range(ds_length))\n",
    "n_test = 128\n",
    "batch_size = 16\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[n_test:], indices[:n_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = DataLoader(ds, batch_size=batch_size, sampler=train_sampler)\n",
    "test_loader = DataLoader(ds, batch_size=batch_size, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23_sandwich_cam01_start_905_event_53_seed_117\n",
      "16_cereals_cam01_start_0_event_32_seed_117\n",
      "18_scrambledegg_webcam02_start_723_event_19_seed_117\n",
      "29_scrambledegg_stereo_1_start_480_event_30_seed_117\n",
      "17_pancake_stereo_1_start_468_event_62_seed_117\n",
      "12_salat_webcam01_start_1923_event_40_seed_117\n",
      "12_friedegg_stereo_1_start_222_event_42_seed_117\n",
      "22_pancake_cam01_start_2114_event_23_seed_117\n",
      "24_coffee_stereo_1_start_115_event_27_seed_117\n",
      "30_scrambledegg_cam01_start_1825_event_8_seed_117\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, path) in enumerate(train_loader):\n",
    "        if batch_idx is 1:\n",
    "            print(Path(path[0]).stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport lab.breakfast.prednet\n",
    "from lab.breakfast.prednet import PredNet\n",
    "%aimport lab.breakfast.lstm\n",
    "from lab.breakfast.lstm import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "A_channels = (1, 32, 48, 96)\n",
    "R_channels = (1, 32, 48, 96)\n",
    "lr = 0.001 # if epoch < 75 else 0.0001\n",
    "nt = 64 # num of time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch CUDA Version Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch 1.3 Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nThe NVIDIA driver on your system is too old (found version 9010).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2429268f434b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlayer_loss_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtime_loss_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtime_loss_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtime_loss_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_loss_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/abdullah/envs/bk/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m             raise RuntimeError(\n\u001b[1;32m    191\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/abdullah/envs/bk/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0mAlternatively\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgo\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m \u001b[0mto\u001b[0m \u001b[0minstall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0ma\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mcompiled\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0myour\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m of the CUDA driver.\"\"\".format(str(torch._C._cuda_getDriverVersion())))\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nThe NVIDIA driver on your system is too old (found version 9010).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver."
     ]
    }
   ],
   "source": [
    "layer_loss_weights = Variable(torch.FloatTensor([[1.], [0.], [0.], [0.]]).cuda())\n",
    "time_loss_weights = 1./(nt - 1) * torch.ones(nt, 1)\n",
    "time_loss_weights[0] = 0\n",
    "time_loss_weights = Variable(time_loss_weights.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9010"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch._C._cuda_getDriverVersion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing pytorch version `1.2` to see if that helps the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch 1.2 Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nThe NVIDIA driver on your system is too old (found version 9010).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2429268f434b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlayer_loss_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtime_loss_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtime_loss_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtime_loss_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_loss_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/abdullah/envs/bk/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m         raise RuntimeError(\n\u001b[1;32m    177\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/abdullah/envs/bk/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0mAlternatively\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgo\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m \u001b[0mto\u001b[0m \u001b[0minstall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0ma\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mcompiled\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0myour\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m of the CUDA driver.\"\"\".format(str(torch._C._cuda_getDriverVersion())))\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nThe NVIDIA driver on your system is too old (found version 9010).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver."
     ]
    }
   ],
   "source": [
    "layer_loss_weights = Variable(torch.FloatTensor([[1.], [0.], [0.], [0.]]).cuda())\n",
    "time_loss_weights = 1./(nt - 1) * torch.ones(nt, 1)\n",
    "time_loss_weights[0] = 0\n",
    "time_loss_weights = Variable(time_loss_weights.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cudatoolkit 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This couldn't work using the `miniconda` setup the lab uses. Got these errors when trying to uninstall, or bump versions:\n",
    "\n",
    "```\n",
    "ERROR conda.core.link:_execute(700): An error occurred while uninstalling package 'pytorch::pytorch-1.2.0-py3.6_cuda10.0.130_cudnn7.6.2_0'.\n",
    "Rolling back transaction: done\n",
    "\n",
    "[Errno 13] Permission denied: '/media/data/conda/abdullah/envs/bk/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so' -> '/media/data/conda/abdullah/envs/bk/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so.c~'\n",
    "()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CUDA 10\n",
    "\n",
    "Problem was vaguely that the machines were running too low of a CUDA version. Machines with CUDA 10 had no problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_loss_weights = Variable(torch.FloatTensor([[1.], [0.], [0.], [0.]]).cuda())\n",
    "# time_loss_weights = 1./(nt - 1) * torch.ones(nt, 1)\n",
    "# time_loss_weights[0] = 0\n",
    "# time_loss_weights = Variable(time_loss_weights.cuda())\n",
    "\n",
    "layer_loss_weights = Variable(torch.FloatTensor([[1.], [0.], [0.], [0.]]))\n",
    "time_loss_weights = 1./(nt - 1) * torch.ones(nt, 1)\n",
    "time_loss_weights[0] = 0\n",
    "time_loss_weights = Variable(time_loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PredNet(R_channels, A_channels, output_mode='error')\n",
    "# if torch.cuda.is_available():\n",
    "#     print('Using GPU.')\n",
    "#     model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def lr_scheduler(optimizer, epoch):\n",
    "    if epoch < num_epochs //2:\n",
    "        return optimizer\n",
    "    else:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.0001\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    optimizer = lr_scheduler(optimizer, epoch)\n",
    "    for batch_idx, (data, path) in tqdm(enumerate(train_loader)):\n",
    "        data = Variable(data)\n",
    "        errors = model(data) # batch x n_layers x nt\n",
    "        loc_batch = errors.size(0)\n",
    "        errors = torch.mm(errors.view(-1, nt), time_loss_weights) # batch*n_layers x 1\n",
    "        errors = torch.mm(errors.view(loc_batch, -1), layer_loss_weights)\n",
    "        errors = torch.mean(errors)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        errors.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if i%10 == 0:\n",
    "            print(f'Epoch: {epoch}/{num_epochs}, step: {i}/{len(ds)//batch_size}, '\n",
    "                  f'errors: {errors.data[0]}')\n",
    "            \n",
    "        break\n",
    "    break\n",
    "#torch.save(model.state_dict(), 'training.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Everything in One Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llw torch.Size([4, 1]) tlw torch.Size([64, 1])\n",
      "torch.Size([8, 4, 64])\n",
      "8\n",
      "torch.Size([32, 1])\n",
      "torch.Size([8, 1])\n",
      "torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:03, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, step: 0/882, errors: 1.7341536283493042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "ds_length = len(ds)\n",
    "indices = list(range(ds_length))\n",
    "n_test = 128\n",
    "batch_size = 8\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[n_test:], indices[:n_test]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = DataLoader(ds, batch_size=batch_size, sampler=train_sampler)\n",
    "test_loader = DataLoader(ds, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "num_epochs = 10\n",
    "A_channels = (64, 128, 256, 512)\n",
    "R_channels = (64, 128, 256, 512)\n",
    "\n",
    "lr = 0.001 # if epoch < 75 else 0.0001\n",
    "nt = 64 # num of time steps\n",
    "\n",
    "layer_loss_weights = Variable(torch.FloatTensor([[1.], [0.], [0.], [0.]]))\n",
    "time_loss_weights = 1./(nt - 1) * torch.ones(nt, 1)\n",
    "time_loss_weights[0] = 0\n",
    "time_loss_weights = Variable(time_loss_weights)\n",
    "\n",
    "print('llw', layer_loss_weights.shape, 'tlw', time_loss_weights.shape)\n",
    "\n",
    "model = PredNet(R_channels, A_channels, output_mode='error')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def lr_scheduler(optimizer, epoch):\n",
    "    if epoch < num_epochs //2:\n",
    "        return optimizer\n",
    "    else:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.0001\n",
    "        return optimizer\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    optimizer = lr_scheduler(optimizer, epoch)\n",
    "    for batch_idx, (data, path) in tqdm(enumerate(train_loader)):\n",
    "        data = Variable(data)\n",
    "        errors = model(data) # batch x n_layers x nt\n",
    "        loc_batch = errors.size(0)\n",
    "        errors = torch.mm(errors.view(-1, nt), time_loss_weights) # batch*n_layers x 1\n",
    "        errors = torch.mm(errors.view(loc_batch, -1), layer_loss_weights)\n",
    "        errors = torch.mean(errors, axis=0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        errors.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch: {epoch}/{num_epochs}, step: {batch_idx}/{len(ds)//batch_size}, '\n",
    "                  f'errors: {errors.data[0]}')\n",
    "            \n",
    "        break\n",
    "    break\n",
    "#torch.save(model.state_dict(), 'training.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 128, 256, 0)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.r_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * model.a_channels[2] + model.r_channels[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.a_channels[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.r_channels[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_layers - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
