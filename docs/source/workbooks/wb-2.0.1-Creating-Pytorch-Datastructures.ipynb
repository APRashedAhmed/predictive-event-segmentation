{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0.1 Creating Pytorch Datastructures\n",
    "\n",
    "Turning the Schapiro graph and walking algorithms into code amenable to pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [watermark](https://github.com/rasbt/watermark) to see the state of the machine and environment that's running the notebook. To make sense of the options, take a look at the [usage](https://github.com/rasbt/watermark#usage) section of the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Tue Aug 18 2020 22:00:18 \n",
      "\n",
      "compiler   : GCC 7.3.0\n",
      "system     : Linux\n",
      "release    : 5.4.0-42-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 4\n",
      "interpreter: 64bit\n",
      "host name  : apra-x3\n",
      "Git hash   : 9dcef66145f8f506529041f0eb7a84a740a26a16\n",
      "Git branch : master\n"
     ]
    }
   ],
   "source": [
    "# Load `watermark` extension\n",
    "%load_ext watermark\n",
    "# Display the status of the machine and other non-code related info\n",
    "%watermark -n -m -g -b -t -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load [autoreload](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html) which will always reload modules marked with `%aimport`.\n",
    "\n",
    "This behavior can be inverted by running `autoreload 2` which will set everything to be auto-reloaded *except* for modules marked with `%aimport`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `autoreload` extension\n",
    "%load_ext autoreload\n",
    "# Set autoreload behavior\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `matplotlib` in one of the more `jupyter`-friendly [rich-output modes](https://ipython.readthedocs.io/en/stable/interactive/plotting.html). Some options (that may or may not have worked) are `inline`, `notebook`, and `gtk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the matplotlib mode\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "networkx  2.4\n",
      "logging   0.5.1.2\n",
      "prevseg   0+untagged.30.g179141d.dirty\n",
      "PIL.Image 7.2.0\n",
      "torch     1.6.0\n",
      "numpy     1.19.1\n",
      "CPython 3.8.5\n",
      "IPython 7.16.1\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
    "\n",
    "%aimport prevseg.index\n",
    "import prevseg.index as index\n",
    "%aimport prevseg.schapiro\n",
    "from prevseg.schapiro import walk, graph\n",
    "\n",
    "# Keep track of versions of everything\n",
    "%watermark -v -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fractal Dataloader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mapping as follows:\n",
      "{0: '90', 1: '99', 2: '70', 3: '74', 4: '35', 5: '47', 6: '62', 7: '29', 8: '49', 9: '89', 10: '81', 11: '41', 12: '32', 13: '1', 14: '13'}\n",
      "0 tensor([1, 2])\n",
      "1 tensor([4, 4])\n",
      "2 tensor([2, 5])\n",
      "0 tensor([ 2, 10])\n",
      "1 tensor([ 0, 13])\n",
      "2 tensor([14, 10])\n",
      "0 tensor([9, 0])\n",
      "1 tensor([7, 1])\n",
      "2 tensor([6, 2])\n"
     ]
    }
   ],
   "source": [
    "max_steps = 3\n",
    "class ShapiroFractalsDataset(IterableDataset):\n",
    "    def __init__(self, batch_size=2, n_pentagons=3, max_steps=max_steps):\n",
    "        self.batch_size = batch_size\n",
    "        self.n_pentagons = n_pentagons\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        self.G = graph.schapiro_graph(n_pentagons=n_pentagons)\n",
    "        \n",
    "        self.load_node_stimuli()\n",
    "        \n",
    "        self.mapping = {node : path.stem for node, path in zip(range(len(self.G.nodes)),\n",
    "                                                               self.paths_data)}\n",
    "        print(f'Created mapping as follows:\\n{self.mapping}')\n",
    "        \n",
    "    def load_node_stimuli(self):\n",
    "        # Load the fractal images into memory\n",
    "        assert index.DIR_SCH_FRACTALS.exists()\n",
    "        paths_data = list(pes.index.DIR_SCH_FRACTALS.iterdir())\n",
    "        np.random.shuffle(paths_data)\n",
    "        self.paths_data = paths_data[:5*self.n_pentagons]\n",
    "        self.array_data = np.array(\n",
    "            [np.array(ImageOps.grayscale(Image.open(str(path))))\n",
    "             for path in self.paths_data])\n",
    "        \n",
    "    def iter_single_sample(self):\n",
    "        for sample in walk.walk_random(self.G, steps=self.max_steps):\n",
    "            #yield self.array_data[sample[0]]\n",
    "            yield sample[0]\n",
    "        \n",
    "    def iter_batch_dataset(self):\n",
    "        batch = zip(*[self.iter_single_sample() for _ in range(self.batch_size)])\n",
    "        try:\n",
    "            while True:\n",
    "                yield np.array(next(batch))\n",
    "        except StopIteration:\n",
    "            return\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self.iter_batch_dataset()\n",
    "    \n",
    "iter_ds = ShapiroFractalsDataset()\n",
    "loader = DataLoader(iter_ds, batch_size=None)\n",
    "epochs = 3\n",
    "for _ in range(epochs):\n",
    "    for i, batch in enumerate(loader):\n",
    "        #print(i, batch.shape)\n",
    "        print(i, batch)\n",
    "        if i > max_steps+5:\n",
    "            print('bad')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2048])\n"
     ]
    }
   ],
   "source": [
    "class ShapiroResnetEmbeddingDataset(ShapiroFractalsDataset):\n",
    "    def load_node_stimuli(self):\n",
    "        # Load the fractal images into memory\n",
    "        assert index.DIR_SCH_FRACTALS.exists()\n",
    "        paths_data = list(pes.index.DIR_SCH_FRACTALS_EMB.iterdir())\n",
    "        np.random.shuffle(paths_data)\n",
    "        self.paths_data = paths_data[:5*self.n_pentagons]\n",
    "        self.array_data = np.array(\n",
    "            [np.array(np.load(str(path)))\n",
    "             for path in self.paths_data])\n",
    "\n",
    "iter_ds = ShapiroResnetEmbeddingDataset()\n",
    "loader = DataLoader(iter_ds, batch_size=None)\n",
    "for batch in loader:\n",
    "    print(batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mapping as follows:\n",
      "{0: '40', 1: '46', 2: '54', 3: '3', 4: '94', 5: '18', 6: '4', 7: '13', 8: '82', 9: '19', 10: '7', 11: '11', 12: '64', 13: '38', 14: '41'}\n",
      "0 torch.Size([2])\n",
      "1 torch.Size([2])\n",
      "2 torch.Size([2])\n",
      "0 torch.Size([2])\n",
      "1 torch.Size([2])\n",
      "2 torch.Size([2])\n",
      "0 torch.Size([2])\n",
      "1 torch.Size([2])\n",
      "2 torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "%aimport prevseg.dataloaders.schapiro\n",
    "import prevseg.dataloaders.schapiro as schapiro\n",
    "\n",
    "max_steps=3\n",
    "\n",
    "iter_ds = ShapiroFractalsDataset(batch_size=2, max_steps=max_steps)\n",
    "loader = DataLoader(iter_ds, batch_size=None)\n",
    "\n",
    "epochs = 3\n",
    "for _ in range(epochs):\n",
    "    for i, batch in enumerate(loader):\n",
    "        print(i, batch.shape)\n",
    "        if i > max_steps+5:\n",
    "            print('bad')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
